import Pkg
Pkg.activate(joinpath(@__DIR__, ".."))   # activates the repo root as the project
Pkg.instantiate()
include("../src_jl/solver_wrappers.jl")
using PowerModels
Base.eval(PowerModels, :(Memento.setlevel!(Memento.getlogger(PowerModels), "error")))
using InfrastructureModels
using Mosek
using MosekTools
using CSV
using DataFrames
using Random
using JSON
using Dates
using Printf
using .SolverWrappers
# ========== å¯é€‰ï¼šæŸ¥çœ‹å†…å­˜ï¼ˆä¸»æµç¨‹ä¸è°ƒç”¨ï¼‰ ==========
function print_memory_usage()
    println("ğŸ§   Mem usage: ", round(Sys.total_memory() / 1024^3, digits=2), " GB total | ",
            round(Sys.free_memory() / 1024^3, digits=2), " GB free")
end

# ä»è·¯å¾„é‡Œå°½åŠ›è§£æ k ä¸ seedï¼šåŒ¹é… "_0.03_perturbation_301_2" è¿™ç±»ç‰‡æ®µ
function infer_k_seed(strs::Vector{String})
    for s in strs
        m = match(r"_([0-9\.]+)_perturbation_([0-1000]+)_([0-9]+)", s)
        if m !== nothing
            k  = try parse(Float64, m.captures[1]) catch; NaN; end
            id = try parse(Int,     m.captures[2]) catch; -1;  end
            return (k, id)
        end
    end
    return (NaN, -1)
end

# å°å·¥å…·ï¼šæ ¼å¼åŒ– k
_fmt_k(k::Float64) = isnan(k) ? nothing : @sprintf("%.2f", k)

"""
solve(data, model, clique_merging)
æ„å»º chordal-SDP çš„ OPF å¹¶ç”¨ MOSEK æ±‚è§£ã€‚
ï¼ˆä¸å†è®¡ç®—/ä¿å­˜åŸå§‹é‚»æ¥ï¼Œä¸è°ƒç”¨ GCï¼‰
"""
function solve(data, model, clique_merging)
    pm = InfrastructureModels.InitializeInfrastructureModel(
        model, data, PowerModels._pm_global_keys, PowerModels.pm_it_sym
    )
    PowerModels.ref_add_core!(pm.ref)

    nw = collect(InfrastructureModels.nw_ids(pm, pm_it_sym))[1]
    println("Beginning chordal extension (merge = $(clique_merging))")

    # ä»…åšå¼¦å›¾æ‰©å±• + å›¢åˆ†è§£
    cadj_chordal, lookup_index, sigma = PowerModels._chordal_extension(pm, nw, clique_merging)

    cliques = PowerModels._maximal_cliques(cadj_chordal)
    lookup_bus_index = Dict((reverse(p) for p in pairs(lookup_index)))
    groups = [[lookup_bus_index[gi] for gi in g] for g in cliques]

    pm.ext[:SDconstraintDecomposition] =
        PowerModels._SDconstraintDecomposition(groups, lookup_index, sigma)

    println("Building the OPF")
    PowerModels.build_opf(pm)

    result = optimize_model!(pm, optimizer=Mosek.Optimizer)
    return result
end

# ================= ä¸»è„šæœ¬ =================
current_dir = @__DIR__
println("Current directory: ", current_dir)

# åŸºç¡€æ¡ˆä¾‹ä¸è¾“å…¥ç›®å½•
case_file  = "case2746wop.m"      
case_name  = replace(case_file, ".m" => "")
input_dir  = joinpath("/home/goatoine/Documents/Lanyue/data/load_profiles/", case_name)

# ä¸‰ç§ chordal formulation + æ˜¯å¦åˆå¹¶å›¢
formulations = [Chordal_MFI, Chordal_AMD, Chordal_MD]
merging_opts = [true, false]
alpha_values = [3.0, 4.0, 5.0]
# è§£ææ–‡ä»¶åï¼šcase14_0.30_perturbation_301_2.json
# è¿”å› (k, seed, id)
function parse_k_seed_id_from_filename(fname::String)
    m = match(r"^[A-Za-z0-9]+_([0-9]+(?:\.[0-9]+)?)_perturbation_([0-9]+)_([0-9]+)\.json$", fname)
    if m === nothing
        return (NaN, missing, missing)
    end
    k     = try parse(Float64, m.captures[1]) catch; NaN; end
    seed  = try parse(Int,     m.captures[2]) catch; missing; end   # 301ï¼ˆä¸ç”¨ï¼‰
    idno  = try parse(Int,     m.captures[3]) catch; missing; end   # 2ï¼ˆè¦ç”¨ï¼‰
    return (k, seed, idno)
end


# ç»“æœè¡¨çš„åˆ—åï¼ˆä¸ä½ çš„æ ·ä¾‹å®Œå…¨ä¸€è‡´ï¼‰
function empty_results_df()
    return DataFrame(
        Formulation     = String[],
        perturbation    = String[],
        Case            = String[],
        Merge           = Bool[],
        A_parameter     = Float64[],
        SolveTime       = Float64[],
        Status          = String[],
        objective       = Float64[],
        SolutionStatus  = String[],
        ID              = Int[],
        load_id         = String[],
    )
end


for json_file in readdir(input_dir)
    endswith(json_file, ".json") || continue

    filepath = joinpath(input_dir, json_file)
    println("\nReading scenario: ", filepath)
    loads = JSON.parsefile(filepath)

    # æ¯ä¸ªåœºæ™¯å•ç‹¬å‡†å¤‡ dataï¼ˆé¿å…ç›¸äº’æ±¡æŸ“ï¼‰
    data_path = "/home/goatoine/Documents/Lanyue/data/raw_data/$case_file"
    data      = PowerModels.parse_file(data_path)
    for (_gen_id, gen) in data["gen"]
        println("gen cost before: ", gen["cost"])
        gen["cost"] .= gen["cost"] ./ 1e3
        println("gen cost after: ", gen["cost"])
    end
    for (idd, load) in loads["load"]
        data["load"][idd]["pd"] = load["pd"]
        data["load"][idd]["qd"] = load["qd"]
    end
    # ä»æ–‡ä»¶åè§£æï¼šk/seed/idï¼ˆä»…ä½¿ç”¨ idï¼‰
    k_detect, seed_detect, id_detect = parse_k_seed_id_from_filename(json_file)
    k_token  = _fmt_k(k_detect)  
    seed_token = isnan(seed_detect) ? NaN : seed_detect # "0.30" æˆ– nothing
    id  = (id_detect === missing) ? "" : string(id_detect)  # ä»…ç”¨ idï¼ˆ=2ï¼‰
    # åœºæ™¯æ ‡è¯†
    perturbation_name = replace(json_file, ".json" => "")
    load_id           = perturbation_name
    # A_parameterï¼ˆkï¼‰ï¼šè‹¥èƒ½è¯†åˆ«å†™ kï¼Œå¦åˆ™ NaN
    k_value = (k_token === nothing) ? NaN : parse(Float64, k_token)
    seed_value = (seed_detect === missing) ? NaN : Int(seed_detect)
    perturbation = (k_value, seed_value)
    
    # è¯¥åœºæ™¯çš„"å•ç‹¬ CSV æ–‡ä»¶å"
    # æ ¼å¼ï¼špglib_opf_<case>_k_<k>_<id>_perturbation.csv
    tokens = ["pglib_opf", case_name]
    if k_token !== nothing && !ismissing(id_detect)
        append!(tokens, ["k", k_token, string(id_detect), "perturbation.csv"])
    else
        append!(tokens, ["perturbation.csv"])
    end
    csv_path = "/home/goatoine/Documents/Lanyue/data/solve_time/$case_name"
    !isdir(csv_path) && mkpath(csv_path)
    results_csv = joinpath(csv_path, join(tokens, "_"))
    println("Scene CSV -> ", results_csv)

    # ä¸ºè¯¥åœºæ™¯æ–°å»ºä¸€ä¸ªç©º DataFrameï¼Œå¹¶å…ˆå†™è¡¨å¤´
    df_scene = empty_results_df()
    CSV.write(results_csv, df_scene)  # å†™ç©ºè¡¨å¤´

    # 6 æ¬¡æ±‚è§£ï¼ˆ3 formulation Ã— 2 mergeï¼‰
    for fm in formulations
        for merging in merging_opts
            alpha_range = merging ? alpha_values : [0.0]  # åˆå¹¶æ—¶ç”¨ alphaï¼Œå¦åˆ™ NaN
            for alpha in alpha_range
                try
                    println("Solving with formulation: ", fm, " | merging: ", merging, " | alpha: ", alpha)
                    save_name = "$(case_name)_$(fm)_$(merging)_$(id)"
                    result = SolverWrappers.solve(data, fm, merging, save_name; alpha=alpha)
                    solve_time  = get(result, "solve_time", NaN)
                    term_status = string(get(result, "termination_status", ""))
                    obj_val     = get(result, "objective", NaN)
                    sol_status  = string(get(result, "solution_status", ""))  # å¯èƒ½ä¸å­˜åœ¨

                    row = DataFrame(
                        Formulation     = [string(fm)],
                        perturbation    = [perturbation],
                        Case            = [case_name],
                        Merge           = [merging],
                        A_parameter     = [alpha],
                        SolveTime       = [solve_time],
                        Status          = [term_status],
                        objective       = [obj_val],
                        SolutionStatus  = [sol_status],
                        ID              = [id_detect],  # ä½¿ç”¨ id_detect
                        load_id         = [load_id],
                    )
                    CSV.write(results_csv, row, append=true)

                catch e
                    @warn "Error solving with formulation=$(fm), merging=$(merging), alpha=$(alpha)" e
                    row_err = DataFrame(
                        Formulation     = [string(fm)],
                        perturbation    = [perturbation],
                        Case            = [case_name],
                        Merge           = [merging],
                        A_parameter     = [alpha],
                        SolveTime       = [NaN],
                        Status          = ["error"],
                        objective       = [NaN],
                        SolutionStatus  = [""],
                        ID              = [id_detect],  # ä½¿ç”¨ id_detect
                        load_id         = [load_id],
                    )
                    CSV.write(results_csv, row_err, append=true)
                end
            end
        end
    end
end

println("\nâœ… å®Œæˆã€‚æ¯ä¸ªåœºæ™¯å·²å„è‡ªç”Ÿæˆ 1 ä¸ª CSVã€‚")
