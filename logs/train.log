2025-09-30 12:51:22 | INFO | === Log start 2025-09-30 12:51:22 ===
2025-09-30 12:51:22 | INFO | === Model outline ===
2025-09-30 12:51:22 | INFO | gnn -> ModuleList
2025-09-30 12:51:22 | INFO |   gnn.0 -> GraphConv
2025-09-30 12:51:22 | INFO |     gnn.0.lin -> Linear
2025-09-30 12:51:22 | INFO |     gnn.0.drop -> Dropout
2025-09-30 12:51:22 | INFO |     gnn.0.act -> ReLU
2025-09-30 12:51:22 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 12:51:22 | INFO |     gnn.0.proj -> Linear
2025-09-30 12:51:22 | INFO |   gnn.1 -> GraphConv
2025-09-30 12:51:22 | INFO |     gnn.1.lin -> Linear
2025-09-30 12:51:22 | INFO |     gnn.1.drop -> Dropout
2025-09-30 12:51:22 | INFO |     gnn.1.act -> ReLU
2025-09-30 12:51:22 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 12:51:22 | INFO |     gnn.1.proj -> Identity
2025-09-30 12:51:22 | INFO | att_q -> Linear
2025-09-30 12:51:22 | INFO | mlp_24 -> Sequential
2025-09-30 12:51:22 | INFO |   mlp_24.0 -> Linear
2025-09-30 12:51:22 | INFO |   mlp_24.1 -> ReLU
2025-09-30 12:51:22 | INFO |   mlp_24.2 -> Linear
2025-09-30 12:51:22 | INFO | scalar_head -> Sequential
2025-09-30 12:51:22 | INFO |   scalar_head.0 -> Linear
2025-09-30 12:51:22 | INFO |   scalar_head.1 -> ReLU
2025-09-30 12:51:22 | INFO |   scalar_head.2 -> LayerNorm
2025-09-30 12:51:22 | INFO |   scalar_head.3 -> Linear
2025-09-30 12:51:22 | INFO |   scalar_head.4 -> ReLU
2025-09-30 12:51:22 | INFO |   scalar_head.5 -> Linear
2025-09-30 12:52:59 | INFO | [epoch1] lr=0.001 train_loss=0.557830 | grad_total=9.293e-01 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 3.11e-01, scalar_head.3.weight: 3.04e-01}
2025-09-30 12:53:28 | INFO | [epoch1] val_loss=0.503865
2025-09-30 12:55:06 | INFO | [epoch2] lr=0.001 train_loss=0.550202 | grad_total=1.432e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 6.46e-01, scalar_head.3.weight: 3.89e-01}
2025-09-30 12:55:35 | INFO | [epoch2] val_loss=0.459761
2025-09-30 12:57:12 | INFO | [epoch3] lr=0.001 train_loss=0.545749 | grad_total=1.410e+01 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 7.31e+00, scalar_head.3.weight: 3.53e+00}
2025-09-30 12:57:41 | INFO | [epoch3] val_loss=0.453347
2025-09-30 12:59:18 | INFO | [epoch4] lr=0.001 train_loss=0.539584 | grad_total=1.880e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 9.76e-01, scalar_head.3.weight: 4.56e-01}
2025-09-30 12:59:47 | INFO | [epoch4] val_loss=0.461760
2025-09-30 13:01:24 | INFO | [epoch5] lr=0.001 train_loss=0.544764 | grad_total=4.628e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 2.50e+00, scalar_head.3.weight: 1.07e+00}
2025-09-30 13:01:53 | INFO | [epoch5] val_loss=0.449852
2025-09-30 13:03:31 | INFO | [epoch6] lr=0.001 train_loss=0.540282 | grad_total=7.815e-01 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 4.27e-01, scalar_head.3.weight: 1.74e-01}
2025-09-30 13:03:59 | INFO | [epoch6] val_loss=0.449533
2025-09-30 13:05:37 | INFO | [epoch7] lr=0.001 train_loss=0.536646 | grad_total=3.534e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 1.93e+00, scalar_head.3.weight: 7.58e-01}
2025-09-30 13:06:05 | INFO | [epoch7] val_loss=0.451142
2025-09-30 13:07:43 | INFO | [epoch8] lr=0.001 train_loss=0.539517 | grad_total=7.440e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 4.17e+00, scalar_head.3.weight: 1.47e+00}
2025-09-30 13:08:12 | INFO | [epoch8] val_loss=0.462500
2025-09-30 13:09:49 | INFO | [epoch9] lr=0.001 train_loss=0.544175 | grad_total=4.920e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 2.62e+00, scalar_head.3.weight: 9.66e-01}
2025-09-30 13:10:18 | INFO | [epoch9] val_loss=0.462593
2025-09-30 13:11:56 | INFO | [epoch10] lr=0.001 train_loss=0.534382 | grad_total=9.913e+00 gradNone=6 gradZero=0 | top2 {scalar_head.5.weight: 5.56e+00, scalar_head.3.weight: 1.61e+00}
2025-09-30 13:12:24 | INFO | [epoch10] val_loss=0.468300
2025-09-30 13:14:47 | INFO | === Log start 2025-09-30 13:14:47 ===
2025-09-30 13:14:47 | INFO | === Model outline ===
2025-09-30 13:14:47 | INFO | gnn -> ModuleList
2025-09-30 13:14:47 | INFO |   gnn.0 -> GraphConv
2025-09-30 13:14:47 | INFO |     gnn.0.lin -> Linear
2025-09-30 13:14:47 | INFO |     gnn.0.drop -> Dropout
2025-09-30 13:14:47 | INFO |     gnn.0.act -> ReLU
2025-09-30 13:14:47 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 13:14:47 | INFO |     gnn.0.proj -> Linear
2025-09-30 13:14:47 | INFO |   gnn.1 -> GraphConv
2025-09-30 13:14:47 | INFO |     gnn.1.lin -> Linear
2025-09-30 13:14:47 | INFO |     gnn.1.drop -> Dropout
2025-09-30 13:14:47 | INFO |     gnn.1.act -> ReLU
2025-09-30 13:14:47 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 13:14:47 | INFO |     gnn.1.proj -> Identity
2025-09-30 13:14:47 | INFO | att_q -> Linear
2025-09-30 13:14:47 | INFO | mlp_24 -> Sequential
2025-09-30 13:14:47 | INFO |   mlp_24.0 -> Linear
2025-09-30 13:14:47 | INFO |   mlp_24.1 -> ReLU
2025-09-30 13:14:47 | INFO |   mlp_24.2 -> Linear
2025-09-30 13:14:47 | INFO | scalar_head -> Sequential
2025-09-30 13:14:47 | INFO |   scalar_head.0 -> Linear
2025-09-30 13:14:47 | INFO |   scalar_head.1 -> ReLU
2025-09-30 13:14:47 | INFO |   scalar_head.2 -> Linear
2025-09-30 13:16:24 | INFO | [epoch1] lr=0.001 train_loss=39309978244.237305 | grad_total=3.532e+11 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 2.28e+11, scalar_head.0.weight: 1.25e+11}
2025-09-30 13:16:53 | INFO | [epoch1] val_loss=25810526.476190
2025-09-30 13:18:31 | INFO | [epoch2] lr=0.001 train_loss=4367122.612311 | grad_total=2.185e+10 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.41e+10, scalar_head.0.weight: 7.76e+09}
2025-09-30 13:18:59 | INFO | [epoch2] val_loss=2797.844719
2025-09-30 13:20:39 | INFO | [epoch3] lr=0.001 train_loss=597.995928 | grad_total=1.997e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.29e+09, scalar_head.0.weight: 7.10e+08}
2025-09-30 13:21:07 | INFO | [epoch3] val_loss=101.891521
2025-09-30 13:22:46 | INFO | [epoch4] lr=0.001 train_loss=139.447159 | grad_total=1.482e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 9.56e+08, scalar_head.0.weight: 5.27e+08}
2025-09-30 13:23:15 | INFO | [epoch4] val_loss=104.567627
2025-09-30 13:26:59 | INFO | === Log start 2025-09-30 13:26:59 ===
2025-09-30 13:26:59 | INFO | === Model outline ===
2025-09-30 13:26:59 | INFO | gnn -> ModuleList
2025-09-30 13:26:59 | INFO |   gnn.0 -> GraphConv
2025-09-30 13:26:59 | INFO |     gnn.0.lin -> Linear
2025-09-30 13:26:59 | INFO |     gnn.0.drop -> Dropout
2025-09-30 13:26:59 | INFO |     gnn.0.act -> ReLU
2025-09-30 13:26:59 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 13:26:59 | INFO |     gnn.0.proj -> Linear
2025-09-30 13:26:59 | INFO |   gnn.1 -> GraphConv
2025-09-30 13:26:59 | INFO |     gnn.1.lin -> Linear
2025-09-30 13:26:59 | INFO |     gnn.1.drop -> Dropout
2025-09-30 13:26:59 | INFO |     gnn.1.act -> ReLU
2025-09-30 13:26:59 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 13:26:59 | INFO |     gnn.1.proj -> Identity
2025-09-30 13:26:59 | INFO | att_q -> Linear
2025-09-30 13:26:59 | INFO | mlp_24 -> Sequential
2025-09-30 13:26:59 | INFO |   mlp_24.0 -> Linear
2025-09-30 13:26:59 | INFO |   mlp_24.1 -> ReLU
2025-09-30 13:26:59 | INFO |   mlp_24.2 -> Linear
2025-09-30 13:26:59 | INFO | scalar_head -> Sequential
2025-09-30 13:26:59 | INFO |   scalar_head.0 -> Linear
2025-09-30 13:26:59 | INFO |   scalar_head.1 -> ReLU
2025-09-30 13:26:59 | INFO |   scalar_head.2 -> Linear
2025-09-30 13:27:00 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:00 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:00 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:00 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:01 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:01 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:01 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:01 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:02 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:02 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:02 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:02 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:03 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:03 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:03 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:03 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:04 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:04 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:04 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:04 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:05 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:05 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:05 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:05 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:06 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:06 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:06 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:06 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:08 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:08 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:08 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:08 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:09 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:09 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:09 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:09 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:10 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:10 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:10 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:10 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:11 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:11 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:11 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:11 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:12 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:12 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:12 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:12 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:13 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:13 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:13 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:13 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:14 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:14 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:14 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:14 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:15 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:15 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:15 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:15 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:16 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:16 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:16 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:16 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:18 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:18 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:18 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:18 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:19 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:19 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:19 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:19 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:20 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:20 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:20 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:20 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:21 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:21 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:21 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:21 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:22 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:22 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:22 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:22 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:23 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:23 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:23 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:23 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:24 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:24 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:24 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:24 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:25 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:25 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:25 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:25 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:26 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:26 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:26 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:26 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:28 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:28 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:28 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:28 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:29 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:29 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:29 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:29 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:30 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:30 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:30 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:30 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:31 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:31 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:31 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:31 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:32 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:32 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:32 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:32 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:33 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:33 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:33 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:33 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:34 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:34 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:34 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:34 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:35 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:35 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:35 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:35 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:36 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:36 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:36 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:36 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:38 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:38 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:38 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:38 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:39 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:39 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:39 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:39 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:40 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:40 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:40 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:40 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:41 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:41 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:41 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:41 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:42 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:42 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:42 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:42 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:43 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:43 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:43 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:43 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:44 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:44 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:44 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:44 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:45 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:45 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:45 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:45 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:46 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:46 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:46 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:46 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:48 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:48 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:48 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:48 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:49 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:49 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:49 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:49 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:50 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:50 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:50 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:50 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:51 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:51 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:51 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:51 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:52 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:52 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:52 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:52 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:53 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:53 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:53 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:53 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:54 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:54 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:54 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:54 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:55 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:55 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:55 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:55 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:56 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:56 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:56 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:56 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:58 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:58 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:58 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:58 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:59 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:27:59 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:27:59 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:27:59 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:00 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:00 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:00 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:00 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:01 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:01 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:01 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:01 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:02 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:02 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:02 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:02 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:03 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:03 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:03 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:03 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:04 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:04 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:04 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:04 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:05 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:05 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:05 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:05 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:06 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:06 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:06 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:06 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:08 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:08 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:08 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:08 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:09 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:09 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:09 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:09 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:10 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:10 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:10 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:10 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:11 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=0 gradNone=10
2025-09-30 13:28:11 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:28:11 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:28:11 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:30:04 | INFO | === Log start 2025-09-30 13:30:04 ===
2025-09-30 13:30:04 | INFO | === Model outline ===
2025-09-30 13:30:04 | INFO | gnn -> ModuleList
2025-09-30 13:30:04 | INFO |   gnn.0 -> GraphConv
2025-09-30 13:30:04 | INFO |     gnn.0.lin -> Linear
2025-09-30 13:30:04 | INFO |     gnn.0.drop -> Dropout
2025-09-30 13:30:04 | INFO |     gnn.0.act -> ReLU
2025-09-30 13:30:04 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 13:30:04 | INFO |     gnn.0.proj -> Linear
2025-09-30 13:30:04 | INFO |   gnn.1 -> GraphConv
2025-09-30 13:30:04 | INFO |     gnn.1.lin -> Linear
2025-09-30 13:30:04 | INFO |     gnn.1.drop -> Dropout
2025-09-30 13:30:04 | INFO |     gnn.1.act -> ReLU
2025-09-30 13:30:04 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 13:30:04 | INFO |     gnn.1.proj -> Identity
2025-09-30 13:30:04 | INFO | att_q -> Linear
2025-09-30 13:30:04 | INFO | mlp_24 -> Sequential
2025-09-30 13:30:04 | INFO |   mlp_24.0 -> Linear
2025-09-30 13:30:04 | INFO |   mlp_24.1 -> ReLU
2025-09-30 13:30:04 | INFO |   mlp_24.2 -> Linear
2025-09-30 13:30:04 | INFO | scalar_head -> Sequential
2025-09-30 13:30:04 | INFO |   scalar_head.0 -> Linear
2025-09-30 13:30:04 | INFO |   scalar_head.1 -> ReLU
2025-09-30 13:30:04 | INFO |   scalar_head.2 -> Linear
2025-09-30 13:31:42 | INFO | [epoch1] GRAD[gnn.] total=2.131e+07 params=10 gradNone=0
2025-09-30 13:31:42 | INFO | [epoch1] GRAD[scalar_head.] total=3.532e+11 params=4 gradNone=0
2025-09-30 13:31:42 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:31:42 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:31:42 | INFO | [epoch1] lr=0.001 train_loss=39309978244.237305 | grad_total=3.532e+11 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 2.28e+11, scalar_head.0.weight: 1.25e+11}
2025-09-30 13:32:11 | INFO | [epoch1] val_loss=25810526.476190
2025-09-30 13:33:49 | INFO | [epoch2] GRAD[gnn.] total=1.323e+06 params=10 gradNone=0
2025-09-30 13:33:49 | INFO | [epoch2] GRAD[scalar_head.] total=2.185e+10 params=4 gradNone=0
2025-09-30 13:33:49 | INFO | [epoch2] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 13:33:49 | INFO | [epoch2] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 13:33:49 | INFO | [epoch2] lr=0.001 train_loss=4367122.612311 | grad_total=2.185e+10 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.41e+10, scalar_head.0.weight: 7.76e+09}
2025-09-30 13:34:18 | INFO | [epoch2] val_loss=2797.844719
2025-09-30 17:20:38 | INFO | === Log start 2025-09-30 17:20:38 ===
2025-09-30 17:20:38 | INFO | === Model outline ===
2025-09-30 17:20:38 | INFO | gnn -> ModuleList
2025-09-30 17:20:38 | INFO |   gnn.0 -> GraphConv
2025-09-30 17:20:38 | INFO |     gnn.0.lin -> Linear
2025-09-30 17:20:38 | INFO |     gnn.0.drop -> Dropout
2025-09-30 17:20:38 | INFO |     gnn.0.act -> ReLU
2025-09-30 17:20:38 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 17:20:38 | INFO |     gnn.0.proj -> Linear
2025-09-30 17:20:38 | INFO |   gnn.1 -> GraphConv
2025-09-30 17:20:38 | INFO |     gnn.1.lin -> Linear
2025-09-30 17:20:38 | INFO |     gnn.1.drop -> Dropout
2025-09-30 17:20:38 | INFO |     gnn.1.act -> ReLU
2025-09-30 17:20:38 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 17:20:38 | INFO |     gnn.1.proj -> Identity
2025-09-30 17:20:38 | INFO | att_q -> Linear
2025-09-30 17:20:38 | INFO | mlp_24 -> Sequential
2025-09-30 17:20:38 | INFO |   mlp_24.0 -> Linear
2025-09-30 17:20:38 | INFO |   mlp_24.1 -> ReLU
2025-09-30 17:20:38 | INFO |   mlp_24.2 -> Linear
2025-09-30 17:20:38 | INFO | scalar_head -> Sequential
2025-09-30 17:20:38 | INFO |   scalar_head.0 -> Linear
2025-09-30 17:20:38 | INFO |   scalar_head.1 -> ReLU
2025-09-30 17:20:38 | INFO |   scalar_head.2 -> Linear
2025-09-30 17:39:38 | INFO | [epoch1] GRAD[gnn.] total=2.131e+07 params=10 gradNone=0
2025-09-30 17:39:38 | INFO | [epoch1] GRAD[scalar_head.] total=3.532e+11 params=4 gradNone=0
2025-09-30 17:39:38 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:39:38 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:39:38 | INFO | [epoch1] lr=0.001 train_loss=39309978244.237305 | grad_total=3.532e+11 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 2.28e+11, scalar_head.0.weight: 1.25e+11}
2025-09-30 17:40:06 | INFO | [epoch1] val_loss=25810526.476190
2025-09-30 17:41:44 | INFO | [epoch2] GRAD[gnn.] total=1.323e+06 params=10 gradNone=0
2025-09-30 17:41:44 | INFO | [epoch2] GRAD[scalar_head.] total=2.185e+10 params=4 gradNone=0
2025-09-30 17:41:44 | INFO | [epoch2] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:41:44 | INFO | [epoch2] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:41:44 | INFO | [epoch2] lr=0.001 train_loss=4367122.612311 | grad_total=2.185e+10 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.41e+10, scalar_head.0.weight: 7.76e+09}
2025-09-30 17:42:13 | INFO | [epoch2] val_loss=2797.844719
2025-09-30 17:43:50 | INFO | [epoch3] GRAD[gnn.] total=1.213e+05 params=10 gradNone=0
2025-09-30 17:43:50 | INFO | [epoch3] GRAD[scalar_head.] total=1.997e+09 params=4 gradNone=0
2025-09-30 17:43:50 | INFO | [epoch3] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:43:50 | INFO | [epoch3] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:43:50 | INFO | [epoch3] lr=0.001 train_loss=597.995928 | grad_total=1.997e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.29e+09, scalar_head.0.weight: 7.10e+08}
2025-09-30 17:44:19 | INFO | [epoch3] val_loss=101.891521
2025-09-30 17:45:57 | INFO | [epoch4] GRAD[gnn.] total=8.974e+04 params=10 gradNone=0
2025-09-30 17:45:57 | INFO | [epoch4] GRAD[scalar_head.] total=1.482e+09 params=4 gradNone=0
2025-09-30 17:45:57 | INFO | [epoch4] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:45:57 | INFO | [epoch4] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:45:57 | INFO | [epoch4] lr=0.001 train_loss=139.447159 | grad_total=1.482e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 9.56e+08, scalar_head.0.weight: 5.27e+08}
2025-09-30 17:46:25 | INFO | [epoch4] val_loss=104.567627
2025-09-30 17:48:03 | INFO | [epoch5] GRAD[gnn.] total=1.032e+04 params=10 gradNone=0
2025-09-30 17:48:03 | INFO | [epoch5] GRAD[scalar_head.] total=1.714e+08 params=4 gradNone=0
2025-09-30 17:48:03 | INFO | [epoch5] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:48:03 | INFO | [epoch5] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:48:03 | INFO | [epoch5] lr=0.001 train_loss=119.139778 | grad_total=1.714e+08 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.10e+08, scalar_head.0.weight: 6.09e+07}
2025-09-30 17:48:32 | INFO | [epoch5] val_loss=183.669360
2025-09-30 17:50:09 | INFO | [epoch6] GRAD[gnn.] total=1.305e+05 params=10 gradNone=0
2025-09-30 17:50:09 | INFO | [epoch6] GRAD[scalar_head.] total=2.152e+09 params=4 gradNone=0
2025-09-30 17:50:09 | INFO | [epoch6] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:50:09 | INFO | [epoch6] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:50:09 | INFO | [epoch6] lr=0.001 train_loss=140.017234 | grad_total=2.152e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.39e+09, scalar_head.0.weight: 7.64e+08}
2025-09-30 17:50:38 | INFO | [epoch6] val_loss=158.614248
2025-09-30 17:52:16 | INFO | [epoch7] GRAD[gnn.] total=2.444e+05 params=10 gradNone=0
2025-09-30 17:52:16 | INFO | [epoch7] GRAD[scalar_head.] total=4.024e+09 params=4 gradNone=0
2025-09-30 17:52:16 | INFO | [epoch7] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:52:16 | INFO | [epoch7] GRAD[mlp_24ã€‚.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:52:16 | INFO | [epoch7] lr=0.001 train_loss=123.775596 | grad_total=4.024e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 2.59e+09, scalar_head.0.weight: 1.43e+09}
2025-09-30 17:52:44 | INFO | [epoch7] val_loss=134.887511
2025-09-30 17:54:22 | INFO | [epoch8] GRAD[gnn.] total=1.324e+05 params=10 gradNone=0
2025-09-30 17:54:22 | INFO | [epoch8] GRAD[scalar_head.] total=2.189e+09 params=4 gradNone=0
2025-09-30 17:54:22 | INFO | [epoch8] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:54:22 | INFO | [epoch8] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:54:22 | INFO | [epoch8] lr=0.001 train_loss=143.669212 | grad_total=2.189e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.41e+09, scalar_head.0.weight: 7.78e+08}
2025-09-30 17:54:51 | INFO | [epoch8] val_loss=94.254277
2025-09-30 17:56:29 | INFO | [epoch9] GRAD[gnn.] total=1.562e+04 params=10 gradNone=0
2025-09-30 17:56:29 | INFO | [epoch9] GRAD[scalar_head.] total=2.531e+08 params=4 gradNone=0
2025-09-30 17:56:29 | INFO | [epoch9] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:56:29 | INFO | [epoch9] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:56:29 | INFO | [epoch9] lr=0.001 train_loss=174.757072 | grad_total=2.531e+08 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.63e+08, scalar_head.0.weight: 8.99e+07}
2025-09-30 17:56:57 | INFO | [epoch9] val_loss=185.560270
2025-09-30 17:58:35 | INFO | [epoch10] GRAD[gnn.] total=1.407e+05 params=10 gradNone=0
2025-09-30 17:58:35 | INFO | [epoch10] GRAD[scalar_head.] total=2.335e+09 params=4 gradNone=0
2025-09-30 17:58:35 | INFO | [epoch10] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 17:58:35 | INFO | [epoch10] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 17:58:35 | INFO | [epoch10] lr=0.001 train_loss=146.195839 | grad_total=2.335e+09 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.51e+09, scalar_head.0.weight: 8.29e+08}
2025-09-30 17:59:04 | INFO | [epoch10] val_loss=139.776921
2025-09-30 18:51:15 | INFO | === Log start 2025-09-30 18:51:15 ===
2025-09-30 18:51:15 | INFO | === Model outline ===
2025-09-30 18:51:15 | INFO | gnn -> ModuleList
2025-09-30 18:51:15 | INFO |   gnn.0 -> GraphConv
2025-09-30 18:51:15 | INFO |     gnn.0.lin -> Linear
2025-09-30 18:51:15 | INFO |     gnn.0.drop -> Dropout
2025-09-30 18:51:15 | INFO |     gnn.0.act -> ReLU
2025-09-30 18:51:15 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 18:51:15 | INFO |     gnn.0.proj -> Linear
2025-09-30 18:51:15 | INFO |   gnn.1 -> GraphConv
2025-09-30 18:51:15 | INFO |     gnn.1.lin -> Linear
2025-09-30 18:51:15 | INFO |     gnn.1.drop -> Dropout
2025-09-30 18:51:15 | INFO |     gnn.1.act -> ReLU
2025-09-30 18:51:15 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 18:51:15 | INFO |     gnn.1.proj -> Identity
2025-09-30 18:51:15 | INFO | att_q -> Linear
2025-09-30 18:51:15 | INFO | mlp_24 -> Sequential
2025-09-30 18:51:15 | INFO |   mlp_24.0 -> Linear
2025-09-30 18:51:15 | INFO |   mlp_24.1 -> ReLU
2025-09-30 18:51:15 | INFO |   mlp_24.2 -> Linear
2025-09-30 18:51:15 | INFO | scalar_head -> Sequential
2025-09-30 18:51:15 | INFO |   scalar_head.0 -> Linear
2025-09-30 18:51:15 | INFO |   scalar_head.1 -> ReLU
2025-09-30 18:51:15 | INFO |   scalar_head.2 -> Linear
2025-09-30 18:54:40 | INFO | === Log start 2025-09-30 18:54:40 ===
2025-09-30 18:54:40 | INFO | === Model outline ===
2025-09-30 18:54:40 | INFO | gnn -> ModuleList
2025-09-30 18:54:40 | INFO |   gnn.0 -> GraphConv
2025-09-30 18:54:40 | INFO |     gnn.0.lin -> Linear
2025-09-30 18:54:40 | INFO |     gnn.0.drop -> Dropout
2025-09-30 18:54:40 | INFO |     gnn.0.act -> ReLU
2025-09-30 18:54:40 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 18:54:40 | INFO |     gnn.0.proj -> Linear
2025-09-30 18:54:40 | INFO |   gnn.1 -> GraphConv
2025-09-30 18:54:40 | INFO |     gnn.1.lin -> Linear
2025-09-30 18:54:40 | INFO |     gnn.1.drop -> Dropout
2025-09-30 18:54:40 | INFO |     gnn.1.act -> ReLU
2025-09-30 18:54:40 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 18:54:40 | INFO |     gnn.1.proj -> Identity
2025-09-30 18:54:40 | INFO | att_q -> Linear
2025-09-30 18:54:40 | INFO | mlp_24 -> Sequential
2025-09-30 18:54:40 | INFO |   mlp_24.0 -> Linear
2025-09-30 18:54:40 | INFO |   mlp_24.1 -> ReLU
2025-09-30 18:54:40 | INFO |   mlp_24.2 -> Linear
2025-09-30 18:54:40 | INFO | scalar_head -> Sequential
2025-09-30 18:54:40 | INFO |   scalar_head.0 -> Linear
2025-09-30 18:54:40 | INFO |   scalar_head.1 -> ReLU
2025-09-30 18:54:40 | INFO |   scalar_head.2 -> Linear
2025-09-30 18:54:40 | INFO | z_norm -> LayerNorm
2025-09-30 18:55:37 | INFO | === Log start 2025-09-30 18:55:37 ===
2025-09-30 18:55:37 | INFO | === Model outline ===
2025-09-30 18:55:37 | INFO | gnn -> ModuleList
2025-09-30 18:55:37 | INFO |   gnn.0 -> GraphConv
2025-09-30 18:55:37 | INFO |     gnn.0.lin -> Linear
2025-09-30 18:55:37 | INFO |     gnn.0.drop -> Dropout
2025-09-30 18:55:37 | INFO |     gnn.0.act -> ReLU
2025-09-30 18:55:37 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 18:55:37 | INFO |     gnn.0.proj -> Linear
2025-09-30 18:55:37 | INFO |   gnn.1 -> GraphConv
2025-09-30 18:55:37 | INFO |     gnn.1.lin -> Linear
2025-09-30 18:55:37 | INFO |     gnn.1.drop -> Dropout
2025-09-30 18:55:37 | INFO |     gnn.1.act -> ReLU
2025-09-30 18:55:37 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 18:55:37 | INFO |     gnn.1.proj -> Identity
2025-09-30 18:55:37 | INFO | att_q -> Linear
2025-09-30 18:55:37 | INFO | mlp_24 -> Sequential
2025-09-30 18:55:37 | INFO |   mlp_24.0 -> Linear
2025-09-30 18:55:37 | INFO |   mlp_24.1 -> ReLU
2025-09-30 18:55:37 | INFO |   mlp_24.2 -> Linear
2025-09-30 18:55:37 | INFO | scalar_head -> Sequential
2025-09-30 18:55:37 | INFO |   scalar_head.0 -> Linear
2025-09-30 18:55:37 | INFO |   scalar_head.1 -> ReLU
2025-09-30 18:55:37 | INFO |   scalar_head.2 -> Linear
2025-09-30 18:55:37 | INFO | z_norm -> LayerNorm
2025-09-30 18:57:13 | INFO | [epoch1] GRAD[gnn.] total=1.905e-04 params=10 gradNone=0
2025-09-30 18:57:13 | INFO | [epoch1] GRAD[scalar_head.] total=3.178e+00 params=4 gradNone=0
2025-09-30 18:57:13 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 18:57:13 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 18:57:13 | INFO | [epoch1] lr=0.001 train_loss=0.540247 | grad_total=3.405e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.78e+00, scalar_head.0.weight: 9.99e-01}
2025-09-30 18:57:42 | INFO | [epoch1] val_loss=0.452180
2025-09-30 18:59:18 | INFO | [epoch2] GRAD[gnn.] total=4.027e-05 params=10 gradNone=0
2025-09-30 18:59:18 | INFO | [epoch2] GRAD[scalar_head.] total=5.560e-01 params=4 gradNone=0
2025-09-30 18:59:18 | INFO | [epoch2] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 18:59:18 | INFO | [epoch2] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 18:59:18 | INFO | [epoch2] lr=0.001 train_loss=0.536916 | grad_total=5.993e-01 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 3.07e-01, scalar_head.0.weight: 1.76e-01}
2025-09-30 18:59:46 | INFO | [epoch2] val_loss=0.451534
2025-09-30 19:01:23 | INFO | [epoch3] GRAD[gnn.] total=1.514e-04 params=10 gradNone=0
2025-09-30 19:01:23 | INFO | [epoch3] GRAD[scalar_head.] total=1.729e+00 params=4 gradNone=0
2025-09-30 19:01:23 | INFO | [epoch3] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:01:23 | INFO | [epoch3] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:01:23 | INFO | [epoch3] lr=0.001 train_loss=0.536411 | grad_total=1.869e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 9.50e-01, scalar_head.0.weight: 5.47e-01}
2025-09-30 19:01:51 | INFO | [epoch3] val_loss=0.451944
2025-09-30 19:03:28 | INFO | [epoch4] GRAD[gnn.] total=3.779e-04 params=10 gradNone=0
2025-09-30 19:03:28 | INFO | [epoch4] GRAD[scalar_head.] total=3.726e+00 params=4 gradNone=0
2025-09-30 19:03:28 | INFO | [epoch4] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:03:28 | INFO | [epoch4] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:03:28 | INFO | [epoch4] lr=0.001 train_loss=0.537290 | grad_total=4.037e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 2.03e+00, scalar_head.0.weight: 1.18e+00}
2025-09-30 19:03:56 | INFO | [epoch4] val_loss=0.452278
2025-09-30 19:05:33 | INFO | [epoch5] GRAD[gnn.] total=3.075e-05 params=10 gradNone=0
2025-09-30 19:05:33 | INFO | [epoch5] GRAD[scalar_head.] total=1.982e-01 params=4 gradNone=0
2025-09-30 19:05:33 | INFO | [epoch5] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:05:33 | INFO | [epoch5] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:05:33 | INFO | [epoch5] lr=0.001 train_loss=0.539165 | grad_total=2.161e-01 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.08e-01, scalar_head.0.weight: 6.24e-02}
2025-09-30 19:06:01 | INFO | [epoch5] val_loss=0.450568
2025-09-30 19:07:38 | INFO | [epoch6] GRAD[gnn.] total=5.138e-05 params=10 gradNone=0
2025-09-30 19:07:38 | INFO | [epoch6] GRAD[scalar_head.] total=4.669e-01 params=4 gradNone=0
2025-09-30 19:07:38 | INFO | [epoch6] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:07:38 | INFO | [epoch6] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:07:38 | INFO | [epoch6] lr=0.001 train_loss=0.536229 | grad_total=5.123e-01 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 2.53e-01, scalar_head.0.weight: 1.47e-01}
2025-09-30 19:08:06 | INFO | [epoch6] val_loss=0.450752
2025-09-30 19:09:43 | INFO | [epoch7] GRAD[gnn.] total=3.535e-04 params=10 gradNone=0
2025-09-30 19:09:43 | INFO | [epoch7] GRAD[scalar_head.] total=2.592e+00 params=4 gradNone=0
2025-09-30 19:09:43 | INFO | [epoch7] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:09:43 | INFO | [epoch7] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:09:43 | INFO | [epoch7] lr=0.001 train_loss=0.536426 | grad_total=2.848e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.40e+00, scalar_head.0.weight: 8.10e-01}
2025-09-30 19:10:11 | INFO | [epoch7] val_loss=0.450512
2025-09-30 19:11:48 | INFO | [epoch8] GRAD[gnn.] total=3.437e-04 params=10 gradNone=0
2025-09-30 19:11:48 | INFO | [epoch8] GRAD[scalar_head.] total=2.078e+00 params=4 gradNone=0
2025-09-30 19:11:48 | INFO | [epoch8] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:11:48 | INFO | [epoch8] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:11:48 | INFO | [epoch8] lr=0.001 train_loss=0.537594 | grad_total=2.307e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 1.12e+00, scalar_head.0.weight: 6.48e-01}
2025-09-30 19:12:16 | INFO | [epoch8] val_loss=0.451634
2025-09-30 19:13:53 | INFO | [epoch9] GRAD[gnn.] total=2.243e-04 params=10 gradNone=0
2025-09-30 19:13:53 | INFO | [epoch9] GRAD[scalar_head.] total=1.270e+00 params=4 gradNone=0
2025-09-30 19:13:53 | INFO | [epoch9] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:13:53 | INFO | [epoch9] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:13:53 | INFO | [epoch9] lr=0.001 train_loss=0.536021 | grad_total=1.397e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 6.81e-01, scalar_head.0.weight: 3.92e-01}
2025-09-30 19:14:21 | INFO | [epoch9] val_loss=0.451088
2025-09-30 19:15:58 | INFO | [epoch10] GRAD[gnn.] total=2.148e-04 params=10 gradNone=0
2025-09-30 19:15:58 | INFO | [epoch10] GRAD[scalar_head.] total=1.275e+00 params=4 gradNone=0
2025-09-30 19:15:58 | INFO | [epoch10] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:15:58 | INFO | [epoch10] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:15:58 | INFO | [epoch10] lr=0.001 train_loss=0.535977 | grad_total=1.389e+00 gradNone=6 gradZero=0 | top2 {scalar_head.2.weight: 6.81e-01, scalar_head.0.weight: 3.91e-01}
2025-09-30 19:16:26 | INFO | [epoch10] val_loss=0.456708
2025-09-30 19:46:41 | INFO | === Log start 2025-09-30 19:46:41 ===
2025-09-30 19:46:41 | INFO | === Model outline ===
2025-09-30 19:46:41 | INFO | gnn -> ModuleList
2025-09-30 19:46:41 | INFO |   gnn.0 -> GraphConv
2025-09-30 19:46:41 | INFO |     gnn.0.lin -> Linear
2025-09-30 19:46:41 | INFO |     gnn.0.drop -> Dropout
2025-09-30 19:46:41 | INFO |     gnn.0.act -> ReLU
2025-09-30 19:46:41 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 19:46:41 | INFO |     gnn.0.proj -> Linear
2025-09-30 19:46:41 | INFO |   gnn.1 -> GraphConv
2025-09-30 19:46:41 | INFO |     gnn.1.lin -> Linear
2025-09-30 19:46:41 | INFO |     gnn.1.drop -> Dropout
2025-09-30 19:46:41 | INFO |     gnn.1.act -> ReLU
2025-09-30 19:46:41 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 19:46:41 | INFO |     gnn.1.proj -> Identity
2025-09-30 19:46:41 | INFO | att_q -> Linear
2025-09-30 19:46:41 | INFO | mlp_24 -> Sequential
2025-09-30 19:46:41 | INFO |   mlp_24.0 -> Linear
2025-09-30 19:46:41 | INFO |   mlp_24.1 -> ReLU
2025-09-30 19:46:41 | INFO |   mlp_24.2 -> Linear
2025-09-30 19:46:41 | INFO | scalar_head -> Sequential
2025-09-30 19:46:41 | INFO |   scalar_head.0 -> Linear
2025-09-30 19:46:41 | INFO |   scalar_head.1 -> ReLU
2025-09-30 19:46:41 | INFO |   scalar_head.2 -> Linear
2025-09-30 19:46:41 | INFO | z_norm -> LayerNorm
2025-09-30 19:48:18 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 19:48:18 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 19:48:18 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:48:18 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:48:18 | INFO | [epoch1] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 19:49:37 | INFO | === Log start 2025-09-30 19:49:37 ===
2025-09-30 19:49:37 | INFO | === Model outline ===
2025-09-30 19:49:37 | INFO | gnn -> ModuleList
2025-09-30 19:49:37 | INFO |   gnn.0 -> GraphConv
2025-09-30 19:49:37 | INFO |     gnn.0.lin -> Linear
2025-09-30 19:49:37 | INFO |     gnn.0.drop -> Dropout
2025-09-30 19:49:37 | INFO |     gnn.0.act -> ReLU
2025-09-30 19:49:37 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 19:49:37 | INFO |     gnn.0.proj -> Linear
2025-09-30 19:49:37 | INFO |   gnn.1 -> GraphConv
2025-09-30 19:49:37 | INFO |     gnn.1.lin -> Linear
2025-09-30 19:49:37 | INFO |     gnn.1.drop -> Dropout
2025-09-30 19:49:37 | INFO |     gnn.1.act -> ReLU
2025-09-30 19:49:37 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 19:49:37 | INFO |     gnn.1.proj -> Identity
2025-09-30 19:49:37 | INFO | att_q -> Linear
2025-09-30 19:49:37 | INFO | mlp_24 -> Sequential
2025-09-30 19:49:37 | INFO |   mlp_24.0 -> Linear
2025-09-30 19:49:37 | INFO |   mlp_24.1 -> ReLU
2025-09-30 19:49:37 | INFO |   mlp_24.2 -> Linear
2025-09-30 19:49:37 | INFO | scalar_head -> Sequential
2025-09-30 19:49:37 | INFO |   scalar_head.0 -> Linear
2025-09-30 19:49:37 | INFO |   scalar_head.1 -> ReLU
2025-09-30 19:49:37 | INFO |   scalar_head.2 -> Linear
2025-09-30 19:49:37 | INFO | z_norm -> LayerNorm
2025-09-30 19:51:14 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 19:51:14 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 19:51:14 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 19:51:14 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 19:51:14 | INFO | [epoch1] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 19:59:16 | INFO | === Log start 2025-09-30 19:59:16 ===
2025-09-30 19:59:16 | INFO | === Model outline ===
2025-09-30 19:59:16 | INFO | gnn -> ModuleList
2025-09-30 19:59:16 | INFO |   gnn.0 -> GraphConv
2025-09-30 19:59:16 | INFO |     gnn.0.lin -> Linear
2025-09-30 19:59:16 | INFO |     gnn.0.drop -> Dropout
2025-09-30 19:59:16 | INFO |     gnn.0.act -> ReLU
2025-09-30 19:59:16 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 19:59:16 | INFO |     gnn.0.proj -> Linear
2025-09-30 19:59:16 | INFO |   gnn.1 -> GraphConv
2025-09-30 19:59:16 | INFO |     gnn.1.lin -> Linear
2025-09-30 19:59:16 | INFO |     gnn.1.drop -> Dropout
2025-09-30 19:59:16 | INFO |     gnn.1.act -> ReLU
2025-09-30 19:59:16 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 19:59:16 | INFO |     gnn.1.proj -> Identity
2025-09-30 19:59:16 | INFO | att_q -> Linear
2025-09-30 19:59:16 | INFO | mlp_24 -> Sequential
2025-09-30 19:59:16 | INFO |   mlp_24.0 -> Linear
2025-09-30 19:59:16 | INFO |   mlp_24.1 -> ReLU
2025-09-30 19:59:16 | INFO |   mlp_24.2 -> Linear
2025-09-30 19:59:16 | INFO | scalar_head -> Sequential
2025-09-30 19:59:16 | INFO |   scalar_head.0 -> Linear
2025-09-30 19:59:16 | INFO |   scalar_head.1 -> ReLU
2025-09-30 19:59:16 | INFO |   scalar_head.2 -> Linear
2025-09-30 19:59:16 | INFO | z_norm -> LayerNorm
2025-09-30 20:00:52 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:00:52 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:00:52 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:00:52 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:00:52 | INFO | [epoch1] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:01:21 | INFO | [epoch1] val_loss=2.380242
2025-09-30 20:02:57 | INFO | [epoch2] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:02:57 | INFO | [epoch2] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:02:57 | INFO | [epoch2] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:02:57 | INFO | [epoch2] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:02:57 | INFO | [epoch2] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:03:26 | INFO | [epoch2] val_loss=2.380242
2025-09-30 20:05:02 | INFO | [epoch3] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:05:02 | INFO | [epoch3] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:05:02 | INFO | [epoch3] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:05:02 | INFO | [epoch3] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:05:02 | INFO | [epoch3] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:05:31 | INFO | [epoch3] val_loss=2.380242
2025-09-30 20:07:07 | INFO | [epoch4] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:07:07 | INFO | [epoch4] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:07:07 | INFO | [epoch4] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:07:07 | INFO | [epoch4] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:07:07 | INFO | [epoch4] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:07:36 | INFO | [epoch4] val_loss=2.380242
2025-09-30 20:09:13 | INFO | [epoch5] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:09:13 | INFO | [epoch5] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:09:13 | INFO | [epoch5] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:09:13 | INFO | [epoch5] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:09:13 | INFO | [epoch5] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:09:41 | INFO | [epoch5] val_loss=2.380242
2025-09-30 20:49:54 | INFO | === Log start 2025-09-30 20:49:54 ===
2025-09-30 20:49:54 | INFO | === Model outline ===
2025-09-30 20:49:54 | INFO | gnn -> ModuleList
2025-09-30 20:49:54 | INFO |   gnn.0 -> GraphConv
2025-09-30 20:49:54 | INFO |     gnn.0.lin -> Linear
2025-09-30 20:49:54 | INFO |     gnn.0.drop -> Dropout
2025-09-30 20:49:54 | INFO |     gnn.0.act -> ReLU
2025-09-30 20:49:54 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 20:49:54 | INFO |     gnn.0.proj -> Linear
2025-09-30 20:49:54 | INFO |   gnn.1 -> GraphConv
2025-09-30 20:49:54 | INFO |     gnn.1.lin -> Linear
2025-09-30 20:49:54 | INFO |     gnn.1.drop -> Dropout
2025-09-30 20:49:54 | INFO |     gnn.1.act -> ReLU
2025-09-30 20:49:54 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 20:49:54 | INFO |     gnn.1.proj -> Identity
2025-09-30 20:49:54 | INFO | att_q -> Linear
2025-09-30 20:49:54 | INFO | mlp_24 -> Sequential
2025-09-30 20:49:54 | INFO |   mlp_24.0 -> Linear
2025-09-30 20:49:54 | INFO |   mlp_24.1 -> ReLU
2025-09-30 20:49:54 | INFO |   mlp_24.2 -> Linear
2025-09-30 20:49:54 | INFO | scalar_head -> Sequential
2025-09-30 20:49:54 | INFO |   scalar_head.0 -> Linear
2025-09-30 20:49:54 | INFO |   scalar_head.1 -> ReLU
2025-09-30 20:49:54 | INFO |   scalar_head.2 -> Linear
2025-09-30 20:49:54 | INFO | z_norm -> LayerNorm
2025-09-30 20:52:29 | INFO | [epoch1] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:52:29 | INFO | [epoch1] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:52:29 | INFO | [epoch1] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:52:29 | INFO | [epoch1] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:52:29 | INFO | [epoch1] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:52:58 | INFO | [epoch1] val_loss=2.380242
2025-09-30 20:54:42 | INFO | [epoch2] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:54:42 | INFO | [epoch2] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:54:42 | INFO | [epoch2] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:54:42 | INFO | [epoch2] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:54:42 | INFO | [epoch2] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:55:12 | INFO | [epoch2] val_loss=2.380242
2025-09-30 20:56:53 | INFO | [epoch3] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:56:53 | INFO | [epoch3] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:56:53 | INFO | [epoch3] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:56:53 | INFO | [epoch3] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:56:53 | INFO | [epoch3] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:57:22 | INFO | [epoch3] val_loss=2.380242
2025-09-30 20:59:04 | INFO | [epoch4] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 20:59:04 | INFO | [epoch4] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 20:59:04 | INFO | [epoch4] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 20:59:04 | INFO | [epoch4] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 20:59:04 | INFO | [epoch4] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 20:59:34 | INFO | [epoch4] val_loss=2.380242
2025-09-30 21:01:15 | INFO | [epoch5] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 21:01:15 | INFO | [epoch5] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 21:01:15 | INFO | [epoch5] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 21:01:15 | INFO | [epoch5] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 21:01:15 | INFO | [epoch5] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 21:01:45 | INFO | [epoch5] val_loss=2.380242
2025-09-30 21:03:26 | INFO | [epoch6] GRAD[gnn.] total=0.000e+00 params=10 gradNone=0
2025-09-30 21:03:26 | INFO | [epoch6] GRAD[scalar_head.] total=0.000e+00 params=4 gradNone=0
2025-09-30 21:03:26 | INFO | [epoch6] GRAD[att_q.] total=0.000e+00 params=0 gradNone=2
2025-09-30 21:03:26 | INFO | [epoch6] GRAD[mlp_24.] total=0.000e+00 params=0 gradNone=4
2025-09-30 21:03:26 | INFO | [epoch6] lr=0.001 train_loss=2.375444 | grad_total=0.000e+00 gradNone=8 gradZero=14 | top2 {gnn.0.lin.weight: 0.00e+00, gnn.0.lin.bias: 0.00e+00}
2025-09-30 21:03:56 | INFO | [epoch6] val_loss=2.380242
2025-09-30 21:05:22 | INFO | === Log start 2025-09-30 21:05:22 ===
2025-09-30 21:05:22 | INFO | === Model outline ===
2025-09-30 21:05:22 | INFO | gnn -> ModuleList
2025-09-30 21:05:22 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:05:22 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:05:22 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:05:22 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:05:22 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:05:22 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:05:22 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:05:22 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:05:22 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:05:22 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:05:22 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:05:22 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:05:22 | INFO | att_q -> Linear
2025-09-30 21:05:22 | INFO | mlp_24 -> Sequential
2025-09-30 21:05:22 | INFO |   mlp_24.0 -> Linear
2025-09-30 21:05:22 | INFO |   mlp_24.1 -> ReLU
2025-09-30 21:05:22 | INFO |   mlp_24.2 -> Linear
2025-09-30 21:05:22 | INFO | scalar_head -> Sequential
2025-09-30 21:05:22 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:05:22 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:05:22 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:05:22 | INFO | z_norm -> LayerNorm
2025-09-30 21:08:09 | INFO | === Log start 2025-09-30 21:08:09 ===
2025-09-30 21:08:09 | INFO | === Model outline ===
2025-09-30 21:08:09 | INFO | gnn -> ModuleList
2025-09-30 21:08:09 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:08:09 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:08:09 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:08:09 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:08:09 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:08:09 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:08:09 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:08:09 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:08:09 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:08:09 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:08:09 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:08:09 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:08:09 | INFO | att_q -> Linear
2025-09-30 21:08:09 | INFO | mlp_24 -> Sequential
2025-09-30 21:08:09 | INFO |   mlp_24.0 -> Linear
2025-09-30 21:08:09 | INFO |   mlp_24.1 -> ReLU
2025-09-30 21:08:09 | INFO |   mlp_24.2 -> Linear
2025-09-30 21:08:09 | INFO | scalar_head -> Sequential
2025-09-30 21:08:09 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:08:09 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:08:09 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:08:09 | INFO | z_norm -> LayerNorm
2025-09-30 21:09:48 | INFO | [epoch1] lr=0.001 train_loss=2.375444
2025-09-30 21:10:16 | INFO | [epoch1] val_loss=2.380242
2025-09-30 21:11:55 | INFO | [epoch2] lr=0.001 train_loss=2.375444
2025-09-30 21:26:12 | INFO | === Log start 2025-09-30 21:26:12 ===
2025-09-30 21:26:12 | INFO | === Model outline ===
2025-09-30 21:26:12 | INFO | gnn -> ModuleList
2025-09-30 21:26:12 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:26:12 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:26:12 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:26:12 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:26:12 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:26:12 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:26:12 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:26:12 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:26:12 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:26:12 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:26:12 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:26:12 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:26:12 | INFO | scalar_head -> Sequential
2025-09-30 21:26:12 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:26:12 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:26:12 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:26:12 | INFO | z_norm -> LayerNorm
2025-09-30 21:28:03 | INFO | === Log start 2025-09-30 21:28:03 ===
2025-09-30 21:28:03 | INFO | === Model outline ===
2025-09-30 21:28:03 | INFO | gnn -> ModuleList
2025-09-30 21:28:03 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:28:03 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:28:03 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:28:03 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:28:03 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:28:03 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:28:03 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:28:03 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:28:03 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:28:03 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:28:03 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:28:03 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:28:03 | INFO | scalar_head -> Sequential
2025-09-30 21:28:03 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:28:03 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:28:03 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:28:03 | INFO | z_norm -> LayerNorm
2025-09-30 21:33:02 | INFO | === Log start 2025-09-30 21:33:02 ===
2025-09-30 21:33:02 | INFO | === Model outline ===
2025-09-30 21:33:02 | INFO | gnn -> ModuleList
2025-09-30 21:33:02 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:33:02 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:33:02 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:33:02 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:33:02 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:33:02 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:33:02 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:33:02 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:33:02 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:33:02 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:33:02 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:33:02 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:33:02 | INFO | scalar_head -> Sequential
2025-09-30 21:33:02 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:33:02 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:33:02 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:33:02 | INFO | z_norm -> LayerNorm
2025-09-30 21:36:48 | INFO | === Log start 2025-09-30 21:36:48 ===
2025-09-30 21:36:48 | INFO | === Model outline ===
2025-09-30 21:36:48 | INFO | gnn -> ModuleList
2025-09-30 21:36:48 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:36:48 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:36:48 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:36:48 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:36:48 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:36:48 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:36:48 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:36:48 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:36:48 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:36:48 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:36:48 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:36:48 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:36:48 | INFO | scalar_head -> Sequential
2025-09-30 21:36:48 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:36:48 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:36:48 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:36:48 | INFO | z_norm -> LayerNorm
2025-09-30 21:38:57 | INFO | [epoch1] val_loss=82.432106
2025-09-30 21:41:11 | INFO | [epoch2] val_loss=7.970561
2025-09-30 21:43:31 | INFO | [epoch3] val_loss=21.743316
2025-09-30 21:48:21 | INFO | === Log start 2025-09-30 21:48:21 ===
2025-09-30 21:48:21 | INFO | === Model outline ===
2025-09-30 21:48:21 | INFO | gnn -> ModuleList
2025-09-30 21:48:21 | INFO |   gnn.0 -> GraphConv
2025-09-30 21:48:21 | INFO |     gnn.0.lin -> Linear
2025-09-30 21:48:21 | INFO |     gnn.0.drop -> Dropout
2025-09-30 21:48:21 | INFO |     gnn.0.act -> ReLU
2025-09-30 21:48:21 | INFO |     gnn.0.ln -> LayerNorm
2025-09-30 21:48:21 | INFO |     gnn.0.proj -> Linear
2025-09-30 21:48:21 | INFO |   gnn.1 -> GraphConv
2025-09-30 21:48:21 | INFO |     gnn.1.lin -> Linear
2025-09-30 21:48:21 | INFO |     gnn.1.drop -> Dropout
2025-09-30 21:48:21 | INFO |     gnn.1.act -> ReLU
2025-09-30 21:48:21 | INFO |     gnn.1.ln -> LayerNorm
2025-09-30 21:48:21 | INFO |     gnn.1.proj -> Identity
2025-09-30 21:48:21 | INFO | scalar_head -> Sequential
2025-09-30 21:48:21 | INFO |   scalar_head.0 -> Linear
2025-09-30 21:48:21 | INFO |   scalar_head.1 -> ReLU
2025-09-30 21:48:21 | INFO |   scalar_head.2 -> Linear
2025-09-30 21:48:21 | INFO | z_norm -> LayerNorm
2025-09-30 21:50:02 | INFO | [epoch1] lr=0.001 train_loss=14186.281142 | grad_total=1.907e+05 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 9.96e+04, scalar_head.0.weight: 4.61e+04}
2025-09-30 21:50:31 | INFO | [epoch1] val_loss=82.432106
2025-09-30 22:14:35 | INFO | [epoch2] lr=0.001 train_loss=30.947534 | grad_total=1.498e+05 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 7.69e+04, scalar_head.0.weight: 3.57e+04}
2025-09-30 22:15:04 | INFO | [epoch2] val_loss=7.970561
2025-09-30 22:18:46 | INFO | [epoch3] lr=0.001 train_loss=21.224621 | grad_total=5.797e+04 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 2.98e+04, scalar_head.0.weight: 1.39e+04}
2025-09-30 22:19:15 | INFO | [epoch3] val_loss=21.743316
2025-09-30 22:20:59 | INFO | [epoch4] lr=0.001 train_loss=24.786594 | grad_total=4.162e+04 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 2.14e+04, scalar_head.0.weight: 9.96e+03}
2025-09-30 22:21:28 | INFO | [epoch4] val_loss=16.814953
2025-09-30 22:23:10 | INFO | [epoch5] lr=0.001 train_loss=18.228121 | grad_total=8.269e+04 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 4.25e+04, scalar_head.0.weight: 1.98e+04}
2025-09-30 22:23:39 | INFO | [epoch5] val_loss=1.412175
2025-09-30 22:25:15 | INFO | [epoch6] lr=0.001 train_loss=20.479044 | grad_total=1.544e+05 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 7.97e+04, scalar_head.0.weight: 3.70e+04}
2025-09-30 22:25:44 | INFO | [epoch6] val_loss=5.230399
2025-09-30 22:27:20 | INFO | [epoch7] lr=0.001 train_loss=21.112072 | grad_total=1.032e+05 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 5.34e+04, scalar_head.0.weight: 2.48e+04}
2025-09-30 22:27:49 | INFO | [epoch7] val_loss=1.408087
2025-09-30 22:29:25 | INFO | [epoch8] lr=0.001 train_loss=19.642954 | grad_total=1.188e+05 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 6.15e+04, scalar_head.0.weight: 2.86e+04}
2025-09-30 22:29:54 | INFO | [epoch8] val_loss=0.876925
2025-09-30 22:31:31 | INFO | [epoch9] lr=0.001 train_loss=17.926440 | grad_total=8.948e+04 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 4.63e+04, scalar_head.0.weight: 2.16e+04}
2025-09-30 22:31:59 | INFO | [epoch9] val_loss=6.533921
2025-09-30 22:33:36 | INFO | [epoch10] lr=0.001 train_loss=22.908476 | grad_total=3.549e+05 gradNone=2 gradZero=0 | top2 {scalar_head.2.weight: 1.85e+05, scalar_head.0.weight: 8.59e+04}
2025-09-30 22:34:04 | INFO | [epoch10] val_loss=4.395794
2025-10-10 01:06:16 | INFO | === Log start 2025-10-10 01:06:16 ===
2025-10-10 01:06:16 | INFO | === Model outline ===
2025-10-10 01:06:16 | INFO | gnn -> ModuleList
2025-10-10 01:06:16 | INFO |   gnn.0 -> GraphConv
2025-10-10 01:06:16 | INFO |     gnn.0.lin -> Linear
2025-10-10 01:06:16 | INFO |     gnn.0.drop -> Dropout
2025-10-10 01:06:16 | INFO |     gnn.0.act -> ReLU
2025-10-10 01:06:16 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 01:06:16 | INFO |     gnn.0.proj -> Linear
2025-10-10 01:06:16 | INFO |   gnn.1 -> GraphConv
2025-10-10 01:06:16 | INFO |     gnn.1.lin -> Linear
2025-10-10 01:06:16 | INFO |     gnn.1.drop -> Dropout
2025-10-10 01:06:16 | INFO |     gnn.1.act -> ReLU
2025-10-10 01:06:16 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 01:06:16 | INFO |     gnn.1.proj -> Identity
2025-10-10 01:06:16 | INFO | att_q -> Linear
2025-10-10 01:06:16 | INFO | mlp_arr -> Sequential
2025-10-10 01:06:16 | INFO |   mlp_arr.0 -> Linear
2025-10-10 01:06:16 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 01:06:16 | INFO |   mlp_arr.2 -> Linear
2025-10-10 01:06:16 | INFO | scalar_head -> Sequential
2025-10-10 01:06:16 | INFO |   scalar_head.0 -> Linear
2025-10-10 01:06:16 | INFO |   scalar_head.1 -> ReLU
2025-10-10 01:06:16 | INFO |   scalar_head.2 -> Linear
2025-10-10 01:06:16 | INFO | z_norm -> LayerNorm
2025-10-10 01:08:21 | INFO | [epoch1] lr=0.001 train_loss=2166492966129.055420 | grad_total=1.543e+12 gradNone=4 gradZero=0 | top2 {mlp_arr.0.weight: 8.57e+11, mlp_arr.2.weight: 5.23e+11}
2025-10-10 11:19:22 | INFO | === Log start 2025-10-10 11:19:22 ===
2025-10-10 11:19:22 | INFO | === Model outline ===
2025-10-10 11:19:22 | INFO | gnn -> ModuleList
2025-10-10 11:19:22 | INFO |   gnn.0 -> GraphConv
2025-10-10 11:19:22 | INFO |     gnn.0.lin -> Linear
2025-10-10 11:19:22 | INFO |     gnn.0.drop -> Dropout
2025-10-10 11:19:22 | INFO |     gnn.0.act -> ReLU
2025-10-10 11:19:22 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 11:19:22 | INFO |     gnn.0.proj -> Linear
2025-10-10 11:19:22 | INFO |   gnn.1 -> GraphConv
2025-10-10 11:19:22 | INFO |     gnn.1.lin -> Linear
2025-10-10 11:19:22 | INFO |     gnn.1.drop -> Dropout
2025-10-10 11:19:22 | INFO |     gnn.1.act -> ReLU
2025-10-10 11:19:22 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 11:19:22 | INFO |     gnn.1.proj -> Identity
2025-10-10 11:19:22 | INFO | att_q -> Linear
2025-10-10 11:19:22 | INFO | mlp_arr -> Sequential
2025-10-10 11:19:22 | INFO |   mlp_arr.0 -> Linear
2025-10-10 11:19:22 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 11:19:22 | INFO |   mlp_arr.2 -> Linear
2025-10-10 11:19:22 | INFO | scalar_head -> Sequential
2025-10-10 11:19:22 | INFO |   scalar_head.0 -> Linear
2025-10-10 11:19:22 | INFO |   scalar_head.1 -> ReLU
2025-10-10 11:19:22 | INFO |   scalar_head.2 -> Linear
2025-10-10 11:19:22 | INFO | z_norm -> LayerNorm
2025-10-10 11:25:09 | INFO | [epoch1] lr=0.001 train_loss=2166492966129.055420 | grad_total=1.543e+12 gradNone=4 gradZero=0 | top2 {mlp_arr.0.weight: 8.57e+11, mlp_arr.2.weight: 5.23e+11}
2025-10-10 11:29:57 | INFO | === Log start 2025-10-10 11:29:57 ===
2025-10-10 11:29:57 | INFO | === Model outline ===
2025-10-10 11:29:57 | INFO | gnn -> ModuleList
2025-10-10 11:29:57 | INFO |   gnn.0 -> GraphConv
2025-10-10 11:29:57 | INFO |     gnn.0.lin -> Linear
2025-10-10 11:29:57 | INFO |     gnn.0.drop -> Dropout
2025-10-10 11:29:57 | INFO |     gnn.0.act -> ReLU
2025-10-10 11:29:57 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 11:29:57 | INFO |     gnn.0.proj -> Linear
2025-10-10 11:29:57 | INFO |   gnn.1 -> GraphConv
2025-10-10 11:29:57 | INFO |     gnn.1.lin -> Linear
2025-10-10 11:29:57 | INFO |     gnn.1.drop -> Dropout
2025-10-10 11:29:57 | INFO |     gnn.1.act -> ReLU
2025-10-10 11:29:57 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 11:29:57 | INFO |     gnn.1.proj -> Identity
2025-10-10 11:29:57 | INFO | att_q -> Linear
2025-10-10 11:29:57 | INFO | mlp_arr -> Sequential
2025-10-10 11:29:57 | INFO |   mlp_arr.0 -> Linear
2025-10-10 11:29:57 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 11:29:57 | INFO |   mlp_arr.2 -> Linear
2025-10-10 11:29:57 | INFO | scalar_head -> Sequential
2025-10-10 11:29:57 | INFO |   scalar_head.0 -> Linear
2025-10-10 11:29:57 | INFO |   scalar_head.1 -> ReLU
2025-10-10 11:29:57 | INFO |   scalar_head.2 -> Linear
2025-10-10 11:29:57 | INFO | z_norm -> LayerNorm
2025-10-10 11:30:07 | INFO | === Log start 2025-10-10 11:30:07 ===
2025-10-10 11:30:07 | INFO | === Model outline ===
2025-10-10 11:30:07 | INFO | gnn -> ModuleList
2025-10-10 11:30:07 | INFO |   gnn.0 -> GraphConv
2025-10-10 11:30:07 | INFO |     gnn.0.lin -> Linear
2025-10-10 11:30:07 | INFO |     gnn.0.drop -> Dropout
2025-10-10 11:30:07 | INFO |     gnn.0.act -> ReLU
2025-10-10 11:30:07 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 11:30:07 | INFO |     gnn.0.proj -> Linear
2025-10-10 11:30:07 | INFO |   gnn.1 -> GraphConv
2025-10-10 11:30:07 | INFO |     gnn.1.lin -> Linear
2025-10-10 11:30:07 | INFO |     gnn.1.drop -> Dropout
2025-10-10 11:30:07 | INFO |     gnn.1.act -> ReLU
2025-10-10 11:30:07 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 11:30:07 | INFO |     gnn.1.proj -> Identity
2025-10-10 11:30:07 | INFO | att_q -> Linear
2025-10-10 11:30:07 | INFO | mlp_arr -> Sequential
2025-10-10 11:30:07 | INFO |   mlp_arr.0 -> Linear
2025-10-10 11:30:07 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 11:30:07 | INFO |   mlp_arr.2 -> Linear
2025-10-10 11:30:07 | INFO | scalar_head -> Sequential
2025-10-10 11:30:07 | INFO |   scalar_head.0 -> Linear
2025-10-10 11:30:07 | INFO |   scalar_head.1 -> ReLU
2025-10-10 11:30:07 | INFO |   scalar_head.2 -> Linear
2025-10-10 11:30:07 | INFO | z_norm -> LayerNorm
2025-10-10 11:31:43 | INFO | [epoch1] lr=0.001 train_loss=2166492966129.055420 | grad_total=1.543e+12 gradNone=4 gradZero=0 | top2 {mlp_arr.0.weight: 8.57e+11, mlp_arr.2.weight: 5.23e+11}
2025-10-10 11:36:13 | INFO | === Log start 2025-10-10 11:36:13 ===
2025-10-10 11:36:13 | INFO | === Model outline ===
2025-10-10 11:36:13 | INFO | gnn -> ModuleList
2025-10-10 11:36:13 | INFO |   gnn.0 -> GraphConv
2025-10-10 11:36:13 | INFO |     gnn.0.lin -> Linear
2025-10-10 11:36:13 | INFO |     gnn.0.drop -> Dropout
2025-10-10 11:36:13 | INFO |     gnn.0.act -> ReLU
2025-10-10 11:36:13 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 11:36:13 | INFO |     gnn.0.proj -> Linear
2025-10-10 11:36:13 | INFO |   gnn.1 -> GraphConv
2025-10-10 11:36:13 | INFO |     gnn.1.lin -> Linear
2025-10-10 11:36:13 | INFO |     gnn.1.drop -> Dropout
2025-10-10 11:36:13 | INFO |     gnn.1.act -> ReLU
2025-10-10 11:36:13 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 11:36:13 | INFO |     gnn.1.proj -> Identity
2025-10-10 11:36:13 | INFO | att_q -> Linear
2025-10-10 11:36:13 | INFO | mlp_arr -> Sequential
2025-10-10 11:36:13 | INFO |   mlp_arr.0 -> Linear
2025-10-10 11:36:13 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 11:36:13 | INFO |   mlp_arr.2 -> Linear
2025-10-10 11:36:13 | INFO | scalar_head -> Sequential
2025-10-10 11:36:13 | INFO |   scalar_head.0 -> Linear
2025-10-10 11:36:13 | INFO |   scalar_head.1 -> ReLU
2025-10-10 11:36:13 | INFO |   scalar_head.2 -> Linear
2025-10-10 11:36:13 | INFO | z_norm -> LayerNorm
2025-10-10 11:37:49 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top2 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11}
2025-10-10 11:38:17 | INFO | [epoch1] val_loss=1754383774.205291
2025-10-10 11:39:53 | INFO | [epoch2] lr=0.001 train_loss=191368144.658074 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top2 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09}
2025-10-10 11:40:23 | INFO | [epoch2] val_loss=314594.524967
2025-10-10 11:41:43 | INFO | === Log start 2025-10-10 11:41:43 ===
2025-10-10 11:41:43 | INFO | === Model outline ===
2025-10-10 11:41:43 | INFO | gnn -> ModuleList
2025-10-10 11:41:43 | INFO |   gnn.0 -> GraphConv
2025-10-10 11:41:43 | INFO |     gnn.0.lin -> Linear
2025-10-10 11:41:43 | INFO |     gnn.0.drop -> Dropout
2025-10-10 11:41:43 | INFO |     gnn.0.act -> ReLU
2025-10-10 11:41:43 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 11:41:43 | INFO |     gnn.0.proj -> Linear
2025-10-10 11:41:43 | INFO |   gnn.1 -> GraphConv
2025-10-10 11:41:43 | INFO |     gnn.1.lin -> Linear
2025-10-10 11:41:43 | INFO |     gnn.1.drop -> Dropout
2025-10-10 11:41:43 | INFO |     gnn.1.act -> ReLU
2025-10-10 11:41:43 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 11:41:43 | INFO |     gnn.1.proj -> Identity
2025-10-10 11:41:43 | INFO | att_q -> Linear
2025-10-10 11:41:43 | INFO | mlp_arr -> Sequential
2025-10-10 11:41:43 | INFO |   mlp_arr.0 -> Linear
2025-10-10 11:41:43 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 11:41:43 | INFO |   mlp_arr.2 -> Linear
2025-10-10 11:41:43 | INFO | scalar_head -> Sequential
2025-10-10 11:41:43 | INFO |   scalar_head.0 -> Linear
2025-10-10 11:41:43 | INFO |   scalar_head.1 -> ReLU
2025-10-10 11:41:43 | INFO |   scalar_head.2 -> Linear
2025-10-10 11:41:43 | INFO | z_norm -> LayerNorm
2025-10-10 11:41:53 | INFO | === Log start 2025-10-10 11:41:53 ===
2025-10-10 11:41:53 | INFO | === Model outline ===
2025-10-10 11:41:53 | INFO | gnn -> ModuleList
2025-10-10 11:41:53 | INFO |   gnn.0 -> GraphConv
2025-10-10 11:41:53 | INFO |     gnn.0.lin -> Linear
2025-10-10 11:41:53 | INFO |     gnn.0.drop -> Dropout
2025-10-10 11:41:53 | INFO |     gnn.0.act -> ReLU
2025-10-10 11:41:53 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 11:41:53 | INFO |     gnn.0.proj -> Linear
2025-10-10 11:41:53 | INFO |   gnn.1 -> GraphConv
2025-10-10 11:41:53 | INFO |     gnn.1.lin -> Linear
2025-10-10 11:41:53 | INFO |     gnn.1.drop -> Dropout
2025-10-10 11:41:53 | INFO |     gnn.1.act -> ReLU
2025-10-10 11:41:53 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 11:41:53 | INFO |     gnn.1.proj -> Identity
2025-10-10 11:41:53 | INFO | att_q -> Linear
2025-10-10 11:41:53 | INFO | mlp_arr -> Sequential
2025-10-10 11:41:53 | INFO |   mlp_arr.0 -> Linear
2025-10-10 11:41:53 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 11:41:53 | INFO |   mlp_arr.2 -> Linear
2025-10-10 11:41:53 | INFO | scalar_head -> Sequential
2025-10-10 11:41:53 | INFO |   scalar_head.0 -> Linear
2025-10-10 11:41:53 | INFO |   scalar_head.1 -> ReLU
2025-10-10 11:41:53 | INFO |   scalar_head.2 -> Linear
2025-10-10 11:41:53 | INFO | z_norm -> LayerNorm
2025-10-10 11:43:30 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 11:43:58 | INFO | [epoch1] val_loss=1754383774.205291
2025-10-10 11:45:35 | INFO | [epoch2] lr=0.001 train_loss=191368144.658074 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09, scalar_head.0.weight: 1.60e+09, scalar_head.2.weight: 1.12e+09, gnn.1.lin.weight: 3.33e+05, gnn.0.lin.weight: 1.77e+05, gnn.0.proj.weight: 1.71e+05, gnn.1.ln.bias: 1.45e+05}
2025-10-10 11:46:03 | INFO | [epoch2] val_loss=314594.524967
2025-10-10 11:47:40 | INFO | [epoch3] lr=0.001 train_loss=12098.600288 | grad_total=5.474e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 2.12e+08, scalar_head.0.weight: 1.33e+08, mlp_arr.2.weight: 1.10e+08, scalar_head.2.weight: 9.25e+07, gnn.1.lin.weight: 1.17e+04, gnn.0.lin.weight: 7.64e+03, gnn.0.proj.weight: 6.57e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 11:48:08 | INFO | [epoch3] val_loss=103223.355688
2025-10-10 11:49:45 | INFO | [epoch4] lr=0.001 train_loss=397.064080 | grad_total=4.555e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 1.97e+08, scalar_head.2.weight: 1.37e+08, mlp_arr.0.weight: 7.63e+07, mlp_arr.2.weight: 4.48e+07, gnn.1.lin.weight: 6.45e+03, gnn.0.lin.weight: 4.40e+03, gnn.0.proj.weight: 4.27e+03, gnn.1.ln.bias: 2.68e+03}
2025-10-10 11:50:13 | INFO | [epoch4] val_loss=102091.911152
2025-10-10 11:51:49 | INFO | [epoch5] lr=0.001 train_loss=402.402896 | grad_total=6.400e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.87e+08, scalar_head.2.weight: 2.00e+08, mlp_arr.0.weight: 1.02e+08, mlp_arr.2.weight: 5.15e+07, gnn.1.lin.weight: 9.48e+03, gnn.0.lin.weight: 7.37e+03, gnn.0.proj.weight: 5.54e+03, gnn.1.ln.bias: 4.69e+03}
2025-10-10 11:52:18 | INFO | [epoch5] val_loss=103647.809053
2025-10-10 11:53:54 | INFO | [epoch6] lr=0.001 train_loss=394.974748 | grad_total=5.226e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.32e+08, scalar_head.2.weight: 1.62e+08, mlp_arr.0.weight: 8.43e+07, mlp_arr.2.weight: 4.40e+07, gnn.1.lin.weight: 8.10e+03, gnn.0.lin.weight: 5.39e+03, gnn.0.proj.weight: 5.02e+03, gnn.1.ln.bias: 3.19e+03}
2025-10-10 11:54:23 | INFO | [epoch6] val_loss=103613.029456
2025-10-10 11:55:59 | INFO | [epoch7] lr=0.001 train_loss=398.406310 | grad_total=2.788e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 9.22e+07, mlp_arr.0.weight: 6.98e+07, scalar_head.2.weight: 6.42e+07, mlp_arr.2.weight: 5.27e+07, gnn.1.lin.weight: 4.33e+03, gnn.0.proj.weight: 2.74e+03, gnn.0.lin.weight: 2.16e+03, gnn.0.proj.bias: 1.52e+03}
2025-10-10 11:56:28 | INFO | [epoch7] val_loss=104422.693692
2025-10-10 11:58:04 | INFO | [epoch8] lr=0.001 train_loss=406.077283 | grad_total=2.836e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 1.14e+08, mlp_arr.2.weight: 6.54e+07, scalar_head.0.weight: 6.13e+07, scalar_head.2.weight: 4.27e+07, gnn.1.lin.weight: 4.45e+03, gnn.0.proj.weight: 3.18e+03, gnn.0.lin.weight: 3.15e+03, gnn.1.ln.bias: 2.19e+03}
2025-10-10 11:58:33 | INFO | [epoch8] val_loss=102599.615608
2025-10-10 12:00:09 | INFO | [epoch9] lr=0.001 train_loss=407.512504 | grad_total=8.321e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 4.09e+08, scalar_head.2.weight: 2.85e+08, mlp_arr.0.weight: 8.01e+07, mlp_arr.2.weight: 5.85e+07, gnn.1.lin.weight: 1.15e+04, gnn.0.lin.weight: 8.39e+03, gnn.0.proj.weight: 7.53e+03, gnn.1.ln.bias: 5.16e+03}
2025-10-10 12:00:37 | INFO | [epoch9] val_loss=101935.900331
2025-10-10 12:02:14 | INFO | [epoch10] lr=0.001 train_loss=417.422228 | grad_total=6.277e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 3.01e+08, scalar_head.2.weight: 2.09e+08, mlp_arr.0.weight: 7.53e+07, mlp_arr.2.weight: 4.25e+07, gnn.1.lin.weight: 9.15e+03, gnn.0.lin.weight: 5.08e+03, gnn.0.proj.weight: 5.01e+03, gnn.1.ln.bias: 4.42e+03}
2025-10-10 12:02:43 | INFO | [epoch10] val_loss=103315.758036
2025-10-10 12:04:56 | INFO | === Log start 2025-10-10 12:04:56 ===
2025-10-10 12:04:56 | INFO | === Model outline ===
2025-10-10 12:04:56 | INFO | gnn -> ModuleList
2025-10-10 12:04:56 | INFO |   gnn.0 -> GraphConv
2025-10-10 12:04:56 | INFO |     gnn.0.lin -> Linear
2025-10-10 12:04:56 | INFO |     gnn.0.drop -> Dropout
2025-10-10 12:04:56 | INFO |     gnn.0.act -> ReLU
2025-10-10 12:04:56 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 12:04:56 | INFO |     gnn.0.proj -> Linear
2025-10-10 12:04:56 | INFO |   gnn.1 -> GraphConv
2025-10-10 12:04:56 | INFO |     gnn.1.lin -> Linear
2025-10-10 12:04:56 | INFO |     gnn.1.drop -> Dropout
2025-10-10 12:04:56 | INFO |     gnn.1.act -> ReLU
2025-10-10 12:04:56 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 12:04:56 | INFO |     gnn.1.proj -> Identity
2025-10-10 12:04:56 | INFO | att_q -> Linear
2025-10-10 12:04:56 | INFO | mlp_arr -> Sequential
2025-10-10 12:04:56 | INFO |   mlp_arr.0 -> Linear
2025-10-10 12:04:56 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 12:04:56 | INFO |   mlp_arr.2 -> Linear
2025-10-10 12:04:56 | INFO | scalar_head -> Sequential
2025-10-10 12:04:56 | INFO |   scalar_head.0 -> Linear
2025-10-10 12:04:56 | INFO |   scalar_head.1 -> ReLU
2025-10-10 12:04:56 | INFO |   scalar_head.2 -> Linear
2025-10-10 12:04:56 | INFO | z_norm -> LayerNorm
2025-10-10 12:06:32 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 12:06:38 | INFO | [epoch1] val_loss=1754365685.760000
2025-10-10 12:08:15 | INFO | [epoch2] lr=0.001 train_loss=191368144.658074 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09, scalar_head.0.weight: 1.60e+09, scalar_head.2.weight: 1.12e+09, gnn.1.lin.weight: 3.33e+05, gnn.0.lin.weight: 1.77e+05, gnn.0.proj.weight: 1.71e+05, gnn.1.ln.bias: 1.45e+05}
2025-10-10 12:08:21 | INFO | [epoch2] val_loss=313671.270000
2025-10-10 12:09:58 | INFO | [epoch3] lr=0.001 train_loss=12098.600288 | grad_total=5.474e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 2.12e+08, scalar_head.0.weight: 1.33e+08, mlp_arr.2.weight: 1.10e+08, scalar_head.2.weight: 9.25e+07, gnn.1.lin.weight: 1.17e+04, gnn.0.lin.weight: 7.64e+03, gnn.0.proj.weight: 6.57e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 12:10:03 | INFO | [epoch3] val_loss=102718.097187
2025-10-10 12:11:40 | INFO | [epoch4] lr=0.001 train_loss=397.064080 | grad_total=4.555e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 1.97e+08, scalar_head.2.weight: 1.37e+08, mlp_arr.0.weight: 7.63e+07, mlp_arr.2.weight: 4.48e+07, gnn.1.lin.weight: 6.45e+03, gnn.0.lin.weight: 4.40e+03, gnn.0.proj.weight: 4.27e+03, gnn.1.ln.bias: 2.68e+03}
2025-10-10 12:11:46 | INFO | [epoch4] val_loss=101580.745625
2025-10-10 12:13:23 | INFO | [epoch5] lr=0.001 train_loss=402.402896 | grad_total=6.400e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.87e+08, scalar_head.2.weight: 2.00e+08, mlp_arr.0.weight: 1.02e+08, mlp_arr.2.weight: 5.15e+07, gnn.1.lin.weight: 9.48e+03, gnn.0.lin.weight: 7.37e+03, gnn.0.proj.weight: 5.54e+03, gnn.1.ln.bias: 4.69e+03}
2025-10-10 12:13:29 | INFO | [epoch5] val_loss=103140.077813
2025-10-10 12:15:05 | INFO | [epoch6] lr=0.001 train_loss=394.974748 | grad_total=5.226e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.32e+08, scalar_head.2.weight: 1.62e+08, mlp_arr.0.weight: 8.43e+07, mlp_arr.2.weight: 4.40e+07, gnn.1.lin.weight: 8.10e+03, gnn.0.lin.weight: 5.39e+03, gnn.0.proj.weight: 5.02e+03, gnn.1.ln.bias: 3.19e+03}
2025-10-10 12:15:11 | INFO | [epoch6] val_loss=103105.884063
2025-10-10 12:16:48 | INFO | [epoch7] lr=0.001 train_loss=398.406310 | grad_total=2.788e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 9.22e+07, mlp_arr.0.weight: 6.98e+07, scalar_head.2.weight: 6.42e+07, mlp_arr.2.weight: 5.27e+07, gnn.1.lin.weight: 4.33e+03, gnn.0.proj.weight: 2.74e+03, gnn.0.lin.weight: 2.16e+03, gnn.0.proj.bias: 1.52e+03}
2025-10-10 12:16:54 | INFO | [epoch7] val_loss=103920.990625
2025-10-10 12:18:31 | INFO | [epoch8] lr=0.001 train_loss=406.077283 | grad_total=2.836e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 1.14e+08, mlp_arr.2.weight: 6.54e+07, scalar_head.0.weight: 6.13e+07, scalar_head.2.weight: 4.27e+07, gnn.1.lin.weight: 4.45e+03, gnn.0.proj.weight: 3.18e+03, gnn.0.lin.weight: 3.15e+03, gnn.1.ln.bias: 2.19e+03}
2025-10-10 12:18:37 | INFO | [epoch8] val_loss=102100.651250
2025-10-10 12:20:14 | INFO | [epoch9] lr=0.001 train_loss=407.512504 | grad_total=8.321e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 4.09e+08, scalar_head.2.weight: 2.85e+08, mlp_arr.0.weight: 8.01e+07, mlp_arr.2.weight: 5.85e+07, gnn.1.lin.weight: 1.15e+04, gnn.0.lin.weight: 8.39e+03, gnn.0.proj.weight: 7.53e+03, gnn.1.ln.bias: 5.16e+03}
2025-10-10 12:20:19 | INFO | [epoch9] val_loss=101408.265625
2025-10-10 12:21:56 | INFO | [epoch10] lr=0.001 train_loss=417.422228 | grad_total=6.277e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 3.01e+08, scalar_head.2.weight: 2.09e+08, mlp_arr.0.weight: 7.53e+07, mlp_arr.2.weight: 4.25e+07, gnn.1.lin.weight: 9.15e+03, gnn.0.lin.weight: 5.08e+03, gnn.0.proj.weight: 5.01e+03, gnn.1.ln.bias: 4.42e+03}
2025-10-10 12:22:02 | INFO | [epoch10] val_loss=102809.835000
2025-10-10 12:35:06 | INFO | === Log start 2025-10-10 12:35:06 ===
2025-10-10 12:35:06 | INFO | === Model outline ===
2025-10-10 12:35:06 | INFO | gnn -> ModuleList
2025-10-10 12:35:06 | INFO |   gnn.0 -> GraphConv
2025-10-10 12:35:06 | INFO |     gnn.0.lin -> Linear
2025-10-10 12:35:06 | INFO |     gnn.0.drop -> Dropout
2025-10-10 12:35:06 | INFO |     gnn.0.act -> ReLU
2025-10-10 12:35:06 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 12:35:06 | INFO |     gnn.0.proj -> Linear
2025-10-10 12:35:06 | INFO |   gnn.1 -> GraphConv
2025-10-10 12:35:06 | INFO |     gnn.1.lin -> Linear
2025-10-10 12:35:06 | INFO |     gnn.1.drop -> Dropout
2025-10-10 12:35:06 | INFO |     gnn.1.act -> ReLU
2025-10-10 12:35:06 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 12:35:06 | INFO |     gnn.1.proj -> Identity
2025-10-10 12:35:06 | INFO | att_q -> Linear
2025-10-10 12:35:06 | INFO | mlp_arr -> Sequential
2025-10-10 12:35:06 | INFO |   mlp_arr.0 -> Linear
2025-10-10 12:35:06 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 12:35:06 | INFO |   mlp_arr.2 -> Linear
2025-10-10 12:35:06 | INFO | scalar_head -> Sequential
2025-10-10 12:35:06 | INFO |   scalar_head.0 -> Linear
2025-10-10 12:35:06 | INFO |   scalar_head.1 -> ReLU
2025-10-10 12:35:06 | INFO |   scalar_head.2 -> Linear
2025-10-10 12:35:06 | INFO | z_norm -> LayerNorm
2025-10-10 12:35:19 | INFO | === Log start 2025-10-10 12:35:19 ===
2025-10-10 12:35:19 | INFO | === Model outline ===
2025-10-10 12:35:19 | INFO | gnn -> ModuleList
2025-10-10 12:35:19 | INFO |   gnn.0 -> GraphConv
2025-10-10 12:35:19 | INFO |     gnn.0.lin -> Linear
2025-10-10 12:35:19 | INFO |     gnn.0.drop -> Dropout
2025-10-10 12:35:19 | INFO |     gnn.0.act -> ReLU
2025-10-10 12:35:19 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 12:35:19 | INFO |     gnn.0.proj -> Linear
2025-10-10 12:35:19 | INFO |   gnn.1 -> GraphConv
2025-10-10 12:35:19 | INFO |     gnn.1.lin -> Linear
2025-10-10 12:35:19 | INFO |     gnn.1.drop -> Dropout
2025-10-10 12:35:19 | INFO |     gnn.1.act -> ReLU
2025-10-10 12:35:19 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 12:35:19 | INFO |     gnn.1.proj -> Identity
2025-10-10 12:35:19 | INFO | att_q -> Linear
2025-10-10 12:35:19 | INFO | mlp_arr -> Sequential
2025-10-10 12:35:19 | INFO |   mlp_arr.0 -> Linear
2025-10-10 12:35:19 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 12:35:19 | INFO |   mlp_arr.2 -> Linear
2025-10-10 12:35:19 | INFO | scalar_head -> Sequential
2025-10-10 12:35:19 | INFO |   scalar_head.0 -> Linear
2025-10-10 12:35:19 | INFO |   scalar_head.1 -> ReLU
2025-10-10 12:35:19 | INFO |   scalar_head.2 -> Linear
2025-10-10 12:35:19 | INFO | z_norm -> LayerNorm
2025-10-10 12:36:56 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 12:37:02 | INFO | [epoch1] val_loss=1754365685.760000
2025-10-10 12:37:03 | INFO | [debug] same val batch | eval-mode: 1754231680.000000 | train-mode(no_grad): 1732886144.000000
2025-10-10 12:38:39 | INFO | [epoch2] lr=0.001 train_loss=191369165.015267 | grad_total=1.305e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.43e+09, mlp_arr.2.weight: 4.85e+09, scalar_head.0.weight: 1.04e+09, scalar_head.2.weight: 7.25e+08, gnn.1.lin.weight: 3.41e+05, gnn.0.lin.weight: 1.73e+05, gnn.0.proj.weight: 1.72e+05, gnn.1.ln.bias: 1.51e+05}
2025-10-10 12:38:45 | INFO | [epoch2] val_loss=314470.332500
2025-10-10 12:40:22 | INFO | [epoch3] lr=0.001 train_loss=12062.032134 | grad_total=8.394e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 3.31e+08, scalar_head.2.weight: 2.31e+08, mlp_arr.0.weight: 1.82e+08, mlp_arr.2.weight: 9.55e+07, gnn.1.lin.weight: 1.21e+04, gnn.0.proj.weight: 8.02e+03, gnn.0.lin.weight: 7.43e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 12:40:28 | INFO | [epoch3] val_loss=103462.542813
2025-10-10 12:42:04 | INFO | [epoch4] lr=0.001 train_loss=401.526846 | grad_total=2.478e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 1.47e+08, mlp_arr.2.weight: 6.95e+07, scalar_head.0.weight: 1.83e+07, scalar_head.2.weight: 1.27e+07, gnn.1.lin.weight: 7.90e+03, gnn.0.lin.weight: 5.46e+03, gnn.1.ln.bias: 4.50e+03, gnn.0.proj.weight: 4.49e+03}
2025-10-10 12:42:10 | INFO | [epoch4] val_loss=103313.269687
2025-10-10 12:43:44 | INFO | === Log start 2025-10-10 12:43:44 ===
2025-10-10 12:43:44 | INFO | === Model outline ===
2025-10-10 12:43:44 | INFO | gnn -> ModuleList
2025-10-10 12:43:44 | INFO |   gnn.0 -> GraphConv
2025-10-10 12:43:44 | INFO |     gnn.0.lin -> Linear
2025-10-10 12:43:44 | INFO |     gnn.0.drop -> Dropout
2025-10-10 12:43:44 | INFO |     gnn.0.act -> ReLU
2025-10-10 12:43:44 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 12:43:44 | INFO |     gnn.0.proj -> Linear
2025-10-10 12:43:44 | INFO |   gnn.1 -> GraphConv
2025-10-10 12:43:44 | INFO |     gnn.1.lin -> Linear
2025-10-10 12:43:44 | INFO |     gnn.1.drop -> Dropout
2025-10-10 12:43:44 | INFO |     gnn.1.act -> ReLU
2025-10-10 12:43:44 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 12:43:44 | INFO |     gnn.1.proj -> Identity
2025-10-10 12:43:44 | INFO | att_q -> Linear
2025-10-10 12:43:44 | INFO | mlp_arr -> Sequential
2025-10-10 12:43:44 | INFO |   mlp_arr.0 -> Linear
2025-10-10 12:43:44 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 12:43:44 | INFO |   mlp_arr.2 -> Linear
2025-10-10 12:43:44 | INFO | scalar_head -> Sequential
2025-10-10 12:43:44 | INFO |   scalar_head.0 -> Linear
2025-10-10 12:43:44 | INFO |   scalar_head.1 -> ReLU
2025-10-10 12:43:44 | INFO |   scalar_head.2 -> Linear
2025-10-10 12:43:44 | INFO | z_norm -> LayerNorm
2025-10-10 12:45:20 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 12:45:26 | INFO | [epoch1] val_loss=1754365685.760000
2025-10-10 12:45:28 | INFO | [debug] same val batch | eval-mode: 1754231680.000000 | train-mode(no_grad): 1732886144.000000
2025-10-10 12:47:04 | INFO | [epoch2] lr=0.001 train_loss=191369165.015267 | grad_total=1.305e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.43e+09, mlp_arr.2.weight: 4.85e+09, scalar_head.0.weight: 1.04e+09, scalar_head.2.weight: 7.25e+08, gnn.1.lin.weight: 3.41e+05, gnn.0.lin.weight: 1.73e+05, gnn.0.proj.weight: 1.72e+05, gnn.1.ln.bias: 1.51e+05}
2025-10-10 12:47:10 | INFO | [epoch2] val_loss=314470.332500
2025-10-10 12:48:48 | INFO | [epoch3] lr=0.001 train_loss=12062.032134 | grad_total=8.394e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 3.31e+08, scalar_head.2.weight: 2.31e+08, mlp_arr.0.weight: 1.82e+08, mlp_arr.2.weight: 9.55e+07, gnn.1.lin.weight: 1.21e+04, gnn.0.proj.weight: 8.02e+03, gnn.0.lin.weight: 7.43e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 12:48:55 | INFO | [epoch3] val_loss=103462.542813
2025-10-10 12:49:53 | INFO | === Log start 2025-10-10 12:49:53 ===
2025-10-10 12:49:53 | INFO | === Model outline ===
2025-10-10 12:49:53 | INFO | gnn -> ModuleList
2025-10-10 12:49:53 | INFO |   gnn.0 -> GraphConv
2025-10-10 12:49:53 | INFO |     gnn.0.lin -> Linear
2025-10-10 12:49:53 | INFO |     gnn.0.drop -> Dropout
2025-10-10 12:49:53 | INFO |     gnn.0.act -> ReLU
2025-10-10 12:49:53 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 12:49:53 | INFO |     gnn.0.proj -> Linear
2025-10-10 12:49:53 | INFO |   gnn.1 -> GraphConv
2025-10-10 12:49:53 | INFO |     gnn.1.lin -> Linear
2025-10-10 12:49:53 | INFO |     gnn.1.drop -> Dropout
2025-10-10 12:49:53 | INFO |     gnn.1.act -> ReLU
2025-10-10 12:49:53 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 12:49:53 | INFO |     gnn.1.proj -> Identity
2025-10-10 12:49:53 | INFO | att_q -> Linear
2025-10-10 12:49:53 | INFO | mlp_arr -> Sequential
2025-10-10 12:49:53 | INFO |   mlp_arr.0 -> Linear
2025-10-10 12:49:53 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 12:49:53 | INFO |   mlp_arr.2 -> Linear
2025-10-10 12:49:53 | INFO | scalar_head -> Sequential
2025-10-10 12:49:53 | INFO |   scalar_head.0 -> Linear
2025-10-10 12:49:53 | INFO |   scalar_head.1 -> ReLU
2025-10-10 12:49:53 | INFO |   scalar_head.2 -> Linear
2025-10-10 12:49:53 | INFO | z_norm -> LayerNorm
2025-10-10 12:51:31 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 train_loss_arr=1760373248.000000, train_loss_sca=272464896.000000, train_loss_cls=214596.875000 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 12:51:37 | INFO | [epoch1] val_loss=1754365685.760000, val_loss_arr=1663625088.000000, val_loss_sca=180690720.000000, val_loss_cls=219513.031250
2025-10-10 12:53:14 | INFO | [epoch2] lr=0.001 train_loss=191368144.658074 train_loss_arr=150253.390625, train_loss_sca=1513.541626, train_loss_cls=446.317230 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09, scalar_head.0.weight: 1.60e+09, scalar_head.2.weight: 1.12e+09, gnn.1.lin.weight: 3.33e+05, gnn.0.lin.weight: 1.77e+05, gnn.0.proj.weight: 1.71e+05, gnn.1.ln.bias: 1.45e+05}
2025-10-10 12:53:20 | INFO | [epoch2] val_loss=313671.270000, val_loss_arr=289152.000000, val_loss_sca=40734.136719, val_loss_cls=1650.462280
2025-10-10 12:54:58 | INFO | [epoch3] lr=0.001 train_loss=12098.600288 train_loss_arr=205.353119, train_loss_sca=366.569977, train_loss_cls=53.093613 | grad_total=5.474e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 2.12e+08, scalar_head.0.weight: 1.33e+08, mlp_arr.2.weight: 1.10e+08, scalar_head.2.weight: 9.25e+07, gnn.1.lin.weight: 1.17e+04, gnn.0.lin.weight: 7.64e+03, gnn.0.proj.weight: 6.57e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 12:55:04 | INFO | [epoch3] val_loss=102718.097187, val_loss_arr=85049.460938, val_loss_sca=28961.488281, val_loss_cls=1452.130737
2025-10-10 12:56:41 | INFO | [epoch4] lr=0.001 train_loss=397.064080 train_loss_arr=114.818336, train_loss_sca=292.837250, train_loss_cls=36.929382 | grad_total=4.555e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 1.97e+08, scalar_head.2.weight: 1.37e+08, mlp_arr.0.weight: 7.63e+07, mlp_arr.2.weight: 4.48e+07, gnn.1.lin.weight: 6.45e+03, gnn.0.lin.weight: 4.40e+03, gnn.0.proj.weight: 4.27e+03, gnn.1.ln.bias: 2.68e+03}
2025-10-10 12:56:47 | INFO | [epoch4] val_loss=101580.745625, val_loss_arr=84676.781250, val_loss_sca=27411.265625, val_loss_cls=1456.347900
2025-10-10 12:58:25 | INFO | [epoch5] lr=0.001 train_loss=402.402896 train_loss_arr=205.959473, train_loss_sca=586.977966, train_loss_cls=42.298058 | grad_total=6.400e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.87e+08, scalar_head.2.weight: 2.00e+08, mlp_arr.0.weight: 1.02e+08, mlp_arr.2.weight: 5.15e+07, gnn.1.lin.weight: 9.48e+03, gnn.0.lin.weight: 7.37e+03, gnn.0.proj.weight: 5.54e+03, gnn.1.ln.bias: 4.69e+03}
2025-10-10 12:58:31 | INFO | [epoch5] val_loss=103140.077813, val_loss_arr=84835.070312, val_loss_sca=30244.402344, val_loss_cls=1449.101562
2025-10-10 13:00:15 | INFO | === Log start 2025-10-10 13:00:15 ===
2025-10-10 13:00:15 | INFO | === Model outline ===
2025-10-10 13:00:15 | INFO | gnn -> ModuleList
2025-10-10 13:00:15 | INFO |   gnn.0 -> GraphConv
2025-10-10 13:00:15 | INFO |     gnn.0.lin -> Linear
2025-10-10 13:00:15 | INFO |     gnn.0.drop -> Dropout
2025-10-10 13:00:15 | INFO |     gnn.0.act -> ReLU
2025-10-10 13:00:15 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 13:00:15 | INFO |     gnn.0.proj -> Linear
2025-10-10 13:00:15 | INFO |   gnn.1 -> GraphConv
2025-10-10 13:00:15 | INFO |     gnn.1.lin -> Linear
2025-10-10 13:00:15 | INFO |     gnn.1.drop -> Dropout
2025-10-10 13:00:15 | INFO |     gnn.1.act -> ReLU
2025-10-10 13:00:15 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 13:00:15 | INFO |     gnn.1.proj -> Identity
2025-10-10 13:00:15 | INFO | att_q -> Linear
2025-10-10 13:00:15 | INFO | mlp_arr -> Sequential
2025-10-10 13:00:15 | INFO |   mlp_arr.0 -> Linear
2025-10-10 13:00:15 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 13:00:15 | INFO |   mlp_arr.2 -> Linear
2025-10-10 13:00:15 | INFO | scalar_head -> Sequential
2025-10-10 13:00:15 | INFO |   scalar_head.0 -> Linear
2025-10-10 13:00:15 | INFO |   scalar_head.1 -> ReLU
2025-10-10 13:00:15 | INFO |   scalar_head.2 -> Linear
2025-10-10 13:00:15 | INFO | z_norm -> LayerNorm
2025-10-10 13:01:52 | INFO | [epoch1] lr=0.001 train_loss=2243623552586.000488 train_loss_arr=1760373248.000000, train_loss_sca=272464896.000000, train_loss_cls=214596.875000 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 13:03:17 | INFO | [epoch1] val_loss=1754375625.278307, val_loss_arr=1663627392.000000, val_loss_sca=180638640.000000, val_loss_cls=229569.421875
2025-10-10 13:04:55 | INFO | [epoch2] lr=0.001 train_loss=191368144.658074 train_loss_arr=150253.390625, train_loss_sca=1513.541626, train_loss_cls=446.317230 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09, scalar_head.0.weight: 1.60e+09, scalar_head.2.weight: 1.12e+09, gnn.1.lin.weight: 3.33e+05, gnn.0.lin.weight: 1.77e+05, gnn.0.proj.weight: 1.71e+05, gnn.1.ln.bias: 1.45e+05}
2025-10-10 13:06:20 | INFO | [epoch2] val_loss=314470.742196, val_loss_arr=290303.250000, val_loss_sca=40354.531250, val_loss_cls=1868.939331
2025-10-10 13:07:57 | INFO | [epoch3] lr=0.001 train_loss=12098.600288 train_loss_arr=205.353119, train_loss_sca=366.569977, train_loss_cls=53.093613 | grad_total=5.474e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 2.12e+08, scalar_head.0.weight: 1.33e+08, mlp_arr.2.weight: 1.10e+08, scalar_head.2.weight: 9.25e+07, gnn.1.lin.weight: 1.17e+04, gnn.0.lin.weight: 7.64e+03, gnn.0.proj.weight: 6.57e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 13:09:22 | INFO | [epoch3] val_loss=103233.488539, val_loss_arr=85625.515625, val_loss_sca=28644.181641, val_loss_cls=1613.425781
2025-10-10 13:10:59 | INFO | [epoch4] lr=0.001 train_loss=397.064080 train_loss_arr=114.818336, train_loss_sca=292.837250, train_loss_cls=36.929382 | grad_total=4.555e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 1.97e+08, scalar_head.2.weight: 1.37e+08, mlp_arr.0.weight: 7.63e+07, mlp_arr.2.weight: 4.48e+07, gnn.1.lin.weight: 6.45e+03, gnn.0.lin.weight: 4.40e+03, gnn.0.proj.weight: 4.27e+03, gnn.1.ln.bias: 2.68e+03}
2025-10-10 13:12:24 | INFO | [epoch4] val_loss=102098.825973, val_loss_arr=85257.164062, val_loss_sca=27115.599609, val_loss_cls=1614.063965
2025-10-10 13:14:02 | INFO | [epoch5] lr=0.001 train_loss=402.402896 train_loss_arr=205.959473, train_loss_sca=586.977966, train_loss_cls=42.298058 | grad_total=6.400e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.87e+08, scalar_head.2.weight: 2.00e+08, mlp_arr.0.weight: 1.02e+08, mlp_arr.2.weight: 5.15e+07, gnn.1.lin.weight: 9.48e+03, gnn.0.lin.weight: 7.37e+03, gnn.0.proj.weight: 5.54e+03, gnn.1.ln.bias: 4.69e+03}
2025-10-10 13:15:27 | INFO | [epoch5] val_loss=103654.259248, val_loss_arr=85407.703125, val_loss_sca=29939.740234, val_loss_cls=1607.418579
2025-10-10 13:17:06 | INFO | [epoch6] lr=0.001 train_loss=394.974748 train_loss_arr=161.444427, train_loss_sca=363.012421, train_loss_cls=26.517044 | grad_total=5.226e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.32e+08, scalar_head.2.weight: 1.62e+08, mlp_arr.0.weight: 8.43e+07, mlp_arr.2.weight: 4.40e+07, gnn.1.lin.weight: 8.10e+03, gnn.0.lin.weight: 5.39e+03, gnn.0.proj.weight: 5.02e+03, gnn.1.ln.bias: 3.19e+03}
2025-10-10 13:17:16 | INFO | === Log start 2025-10-10 13:17:16 ===
2025-10-10 13:17:16 | INFO | === Model outline ===
2025-10-10 13:17:16 | INFO | gnn -> ModuleList
2025-10-10 13:17:16 | INFO |   gnn.0 -> GraphConv
2025-10-10 13:17:16 | INFO |     gnn.0.lin -> Linear
2025-10-10 13:17:16 | INFO |     gnn.0.drop -> Dropout
2025-10-10 13:17:16 | INFO |     gnn.0.act -> ReLU
2025-10-10 13:17:16 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 13:17:16 | INFO |     gnn.0.proj -> Linear
2025-10-10 13:17:16 | INFO |   gnn.1 -> GraphConv
2025-10-10 13:17:16 | INFO |     gnn.1.lin -> Linear
2025-10-10 13:17:16 | INFO |     gnn.1.drop -> Dropout
2025-10-10 13:17:16 | INFO |     gnn.1.act -> ReLU
2025-10-10 13:17:16 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 13:17:16 | INFO |     gnn.1.proj -> Identity
2025-10-10 13:17:16 | INFO | att_q -> Linear
2025-10-10 13:17:16 | INFO | mlp_arr -> Sequential
2025-10-10 13:17:16 | INFO |   mlp_arr.0 -> Linear
2025-10-10 13:17:16 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 13:17:16 | INFO |   mlp_arr.2 -> Linear
2025-10-10 13:17:16 | INFO | scalar_head -> Sequential
2025-10-10 13:17:16 | INFO |   scalar_head.0 -> Linear
2025-10-10 13:17:16 | INFO |   scalar_head.1 -> ReLU
2025-10-10 13:17:16 | INFO |   scalar_head.2 -> Linear
2025-10-10 13:17:16 | INFO | z_norm -> LayerNorm
2025-10-10 13:18:53 | INFO | [epoch1] lr=0.001 train_loss=2233390952610.517090 train_loss_arr=1760373248.000000, train_loss_sca=272464896.000000, train_loss_cls=214596.875000 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 13:20:18 | INFO | [epoch1] val_loss=1754375763.415730, val_loss_arr=1663627392.000000, val_loss_sca=180638640.000000, val_loss_cls=229569.421875
2025-10-10 13:21:55 | INFO | [epoch2] lr=0.001 train_loss=190495317.711376 train_loss_arr=150253.390625, train_loss_sca=1513.541626, train_loss_cls=446.317230 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09, scalar_head.0.weight: 1.60e+09, scalar_head.2.weight: 1.12e+09, gnn.1.lin.weight: 3.33e+05, gnn.0.lin.weight: 1.77e+05, gnn.0.proj.weight: 1.71e+05, gnn.1.ln.bias: 1.45e+05}
2025-10-10 13:23:21 | INFO | [epoch2] val_loss=314469.590239, val_loss_arr=290303.250000, val_loss_sca=40354.531250, val_loss_cls=1868.939331
2025-10-10 13:24:58 | INFO | [epoch3] lr=0.001 train_loss=12045.633618 train_loss_arr=205.353119, train_loss_sca=366.569977, train_loss_cls=53.093613 | grad_total=5.474e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 2.12e+08, scalar_head.0.weight: 1.33e+08, mlp_arr.2.weight: 1.10e+08, scalar_head.2.weight: 9.25e+07, gnn.1.lin.weight: 1.17e+04, gnn.0.lin.weight: 7.64e+03, gnn.0.proj.weight: 6.57e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 13:26:23 | INFO | [epoch3] val_loss=103233.219101, val_loss_arr=85625.515625, val_loss_sca=28644.181641, val_loss_cls=1613.425781
2025-10-10 13:28:00 | INFO | [epoch4] lr=0.001 train_loss=396.781218 train_loss_arr=114.818336, train_loss_sca=292.837250, train_loss_cls=36.929382 | grad_total=4.555e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 1.97e+08, scalar_head.2.weight: 1.37e+08, mlp_arr.0.weight: 7.63e+07, mlp_arr.2.weight: 4.48e+07, gnn.1.lin.weight: 6.45e+03, gnn.0.lin.weight: 4.40e+03, gnn.0.proj.weight: 4.27e+03, gnn.1.ln.bias: 2.68e+03}
2025-10-10 13:29:25 | INFO | [epoch4] val_loss=102098.571541, val_loss_arr=85257.164062, val_loss_sca=27115.599609, val_loss_cls=1614.063965
2025-10-10 13:31:02 | INFO | [epoch5] lr=0.001 train_loss=403.232019 train_loss_arr=205.959473, train_loss_sca=586.977966, train_loss_cls=42.298058 | grad_total=6.400e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.87e+08, scalar_head.2.weight: 2.00e+08, mlp_arr.0.weight: 1.02e+08, mlp_arr.2.weight: 5.15e+07, gnn.1.lin.weight: 9.48e+03, gnn.0.lin.weight: 7.37e+03, gnn.0.proj.weight: 5.54e+03, gnn.1.ln.bias: 4.69e+03}
2025-10-10 13:32:27 | INFO | [epoch5] val_loss=103653.976914, val_loss_arr=85407.703125, val_loss_sca=29939.740234, val_loss_cls=1607.418579
2025-10-10 13:34:04 | INFO | [epoch6] lr=0.001 train_loss=394.979358 train_loss_arr=161.444427, train_loss_sca=363.012421, train_loss_cls=26.517044 | grad_total=5.226e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.32e+08, scalar_head.2.weight: 1.62e+08, mlp_arr.0.weight: 8.43e+07, mlp_arr.2.weight: 4.40e+07, gnn.1.lin.weight: 8.10e+03, gnn.0.lin.weight: 5.39e+03, gnn.0.proj.weight: 5.02e+03, gnn.1.ln.bias: 3.19e+03}
2025-10-10 13:35:30 | INFO | [epoch6] val_loss=103620.119294, val_loss_arr=85638.906250, val_loss_sca=29401.115234, val_loss_cls=1609.516235
2025-10-10 13:49:24 | INFO | === Log start 2025-10-10 13:49:24 ===
2025-10-10 13:49:24 | INFO | === Model outline ===
2025-10-10 13:49:24 | INFO | gnn -> ModuleList
2025-10-10 13:49:24 | INFO |   gnn.0 -> GraphConv
2025-10-10 13:49:24 | INFO |     gnn.0.lin -> Linear
2025-10-10 13:49:24 | INFO |     gnn.0.drop -> Dropout
2025-10-10 13:49:24 | INFO |     gnn.0.act -> ReLU
2025-10-10 13:49:24 | INFO |     gnn.0.ln -> LayerNorm
2025-10-10 13:49:24 | INFO |     gnn.0.proj -> Linear
2025-10-10 13:49:24 | INFO |   gnn.1 -> GraphConv
2025-10-10 13:49:24 | INFO |     gnn.1.lin -> Linear
2025-10-10 13:49:24 | INFO |     gnn.1.drop -> Dropout
2025-10-10 13:49:24 | INFO |     gnn.1.act -> ReLU
2025-10-10 13:49:24 | INFO |     gnn.1.ln -> LayerNorm
2025-10-10 13:49:24 | INFO |     gnn.1.proj -> Identity
2025-10-10 13:49:24 | INFO | att_q -> Linear
2025-10-10 13:49:24 | INFO | mlp_arr -> Sequential
2025-10-10 13:49:24 | INFO |   mlp_arr.0 -> Linear
2025-10-10 13:49:24 | INFO |   mlp_arr.1 -> ReLU
2025-10-10 13:49:24 | INFO |   mlp_arr.2 -> Linear
2025-10-10 13:49:24 | INFO | scalar_head -> Sequential
2025-10-10 13:49:24 | INFO |   scalar_head.0 -> Linear
2025-10-10 13:49:24 | INFO |   scalar_head.1 -> ReLU
2025-10-10 13:49:24 | INFO |   scalar_head.2 -> Linear
2025-10-10 13:49:24 | INFO | z_norm -> LayerNorm
2025-10-10 13:51:07 | INFO | [epoch1] lr=0.001 train_loss=2233390952610.517090 train_loss_arr=1760373248.000000, train_loss_sca=272464896.000000, train_loss_cls=214596.875000 | grad_total=2.733e+12 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 8.57e+11, scalar_head.0.weight: 7.98e+11, scalar_head.2.weight: 5.55e+11, mlp_arr.2.weight: 5.23e+11, gnn.1.lin.weight: 4.54e+07, gnn.1.ln.weight: 3.97e+07, gnn.1.ln.bias: 3.68e+07, gnn.0.lin.weight: 2.88e+07}
2025-10-10 13:52:35 | INFO | [epoch1] val_loss=1754375763.415730, val_loss_arr=1663627392.000000, val_loss_sca=180638640.000000, val_loss_cls=229569.421875
2025-10-10 13:54:20 | INFO | [epoch2] lr=0.001 train_loss=190495317.711376 train_loss_arr=150253.390625, train_loss_sca=1513.541626, train_loss_cls=446.317230 | grad_total=1.391e+10 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 6.35e+09, mlp_arr.2.weight: 4.83e+09, scalar_head.0.weight: 1.60e+09, scalar_head.2.weight: 1.12e+09, gnn.1.lin.weight: 3.33e+05, gnn.0.lin.weight: 1.77e+05, gnn.0.proj.weight: 1.71e+05, gnn.1.ln.bias: 1.45e+05}
2025-10-10 13:55:50 | INFO | [epoch2] val_loss=314469.590239, val_loss_arr=290303.250000, val_loss_sca=40354.531250, val_loss_cls=1868.939331
2025-10-10 13:57:33 | INFO | [epoch3] lr=0.001 train_loss=12045.633618 train_loss_arr=205.353119, train_loss_sca=366.569977, train_loss_cls=53.093613 | grad_total=5.474e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 2.12e+08, scalar_head.0.weight: 1.33e+08, mlp_arr.2.weight: 1.10e+08, scalar_head.2.weight: 9.25e+07, gnn.1.lin.weight: 1.17e+04, gnn.0.lin.weight: 7.64e+03, gnn.0.proj.weight: 6.57e+03, gnn.1.ln.bias: 5.08e+03}
2025-10-10 13:59:04 | INFO | [epoch3] val_loss=103233.219101, val_loss_arr=85625.515625, val_loss_sca=28644.181641, val_loss_cls=1613.425781
2025-10-10 14:00:47 | INFO | [epoch4] lr=0.001 train_loss=396.781218 train_loss_arr=114.818336, train_loss_sca=292.837250, train_loss_cls=36.929382 | grad_total=4.555e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 1.97e+08, scalar_head.2.weight: 1.37e+08, mlp_arr.0.weight: 7.63e+07, mlp_arr.2.weight: 4.48e+07, gnn.1.lin.weight: 6.45e+03, gnn.0.lin.weight: 4.40e+03, gnn.0.proj.weight: 4.27e+03, gnn.1.ln.bias: 2.68e+03}
2025-10-10 14:02:18 | INFO | [epoch4] val_loss=102098.571541, val_loss_arr=85257.164062, val_loss_sca=27115.599609, val_loss_cls=1614.063965
2025-10-10 14:04:01 | INFO | [epoch5] lr=0.001 train_loss=403.232019 train_loss_arr=205.959473, train_loss_sca=586.977966, train_loss_cls=42.298058 | grad_total=6.400e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.87e+08, scalar_head.2.weight: 2.00e+08, mlp_arr.0.weight: 1.02e+08, mlp_arr.2.weight: 5.15e+07, gnn.1.lin.weight: 9.48e+03, gnn.0.lin.weight: 7.37e+03, gnn.0.proj.weight: 5.54e+03, gnn.1.ln.bias: 4.69e+03}
2025-10-10 14:05:31 | INFO | [epoch5] val_loss=103653.976914, val_loss_arr=85407.703125, val_loss_sca=29939.740234, val_loss_cls=1607.418579
2025-10-10 14:07:15 | INFO | [epoch6] lr=0.001 train_loss=394.979358 train_loss_arr=161.444427, train_loss_sca=363.012421, train_loss_cls=26.517044 | grad_total=5.226e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 2.32e+08, scalar_head.2.weight: 1.62e+08, mlp_arr.0.weight: 8.43e+07, mlp_arr.2.weight: 4.40e+07, gnn.1.lin.weight: 8.10e+03, gnn.0.lin.weight: 5.39e+03, gnn.0.proj.weight: 5.02e+03, gnn.1.ln.bias: 3.19e+03}
2025-10-10 14:08:45 | INFO | [epoch6] val_loss=103620.119294, val_loss_arr=85638.906250, val_loss_sca=29401.115234, val_loss_cls=1609.516235
2025-10-10 14:10:28 | INFO | [epoch7] lr=0.001 train_loss=398.424402 train_loss_arr=126.951004, train_loss_sca=468.484283, train_loss_cls=20.588312 | grad_total=2.788e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 9.22e+07, mlp_arr.0.weight: 6.98e+07, scalar_head.2.weight: 6.42e+07, mlp_arr.2.weight: 5.27e+07, gnn.1.lin.weight: 4.33e+03, gnn.0.proj.weight: 2.74e+03, gnn.0.lin.weight: 2.16e+03, gnn.0.proj.bias: 1.52e+03}
2025-10-10 14:11:59 | INFO | [epoch7] val_loss=104433.643697, val_loss_arr=85592.585938, val_loss_sca=31121.828125, val_loss_cls=1606.665649
2025-10-10 14:13:42 | INFO | [epoch8] lr=0.001 train_loss=405.762084 train_loss_arr=130.397614, train_loss_sca=272.749451, train_loss_cls=35.126122 | grad_total=2.836e+08 gradNone=4 gradZero=0 | top8 {mlp_arr.0.weight: 1.14e+08, mlp_arr.2.weight: 6.54e+07, scalar_head.0.weight: 6.13e+07, scalar_head.2.weight: 4.27e+07, gnn.1.lin.weight: 4.45e+03, gnn.0.proj.weight: 3.18e+03, gnn.0.lin.weight: 3.15e+03, gnn.1.ln.bias: 2.19e+03}
2025-10-10 14:15:12 | INFO | [epoch8] val_loss=102607.921436, val_loss_arr=84739.773438, val_loss_sca=29144.859375, val_loss_cls=1611.239258
2025-10-10 14:16:55 | INFO | [epoch9] lr=0.001 train_loss=407.543866 train_loss_arr=133.675217, train_loss_sca=459.964264, train_loss_cls=25.362995 | grad_total=8.321e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 4.09e+08, scalar_head.2.weight: 2.85e+08, mlp_arr.0.weight: 8.01e+07, mlp_arr.2.weight: 5.85e+07, gnn.1.lin.weight: 1.15e+04, gnn.0.lin.weight: 8.39e+03, gnn.0.proj.weight: 7.53e+03, gnn.1.ln.bias: 5.16e+03}
2025-10-10 14:18:26 | INFO | [epoch9] val_loss=101941.270190, val_loss_arr=85479.039062, val_loss_sca=26416.388672, val_loss_cls=1600.214722
2025-10-10 14:20:09 | INFO | [epoch10] lr=0.001 train_loss=416.763679 train_loss_arr=98.823082, train_loss_sca=203.375916, train_loss_cls=36.319042 | grad_total=6.277e+08 gradNone=4 gradZero=0 | top8 {scalar_head.0.weight: 3.01e+08, scalar_head.2.weight: 2.09e+08, mlp_arr.0.weight: 7.53e+07, mlp_arr.2.weight: 4.25e+07, gnn.1.lin.weight: 9.15e+03, gnn.0.lin.weight: 5.08e+03, gnn.0.proj.weight: 5.01e+03, gnn.1.ln.bias: 4.42e+03}
2025-10-10 14:21:40 | INFO | [epoch10] val_loss=103325.374386, val_loss_arr=85672.804688, val_loss_sca=28753.953125, val_loss_cls=1612.647217
