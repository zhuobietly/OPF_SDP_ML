
import os, sys, yaml, torch
from pathlib import Path
import random
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))
CONFIG_PATH = PROJECT_ROOT /"configs" / "exp_toy.yaml"
#path :/home/goatoine/Documents/Lanyue/models/GNN/configs/exp_tol.yaml
from model_varients.registry import build as build_model
from graph_builders.original import OriginalGraph
try:
    from features.loads_phys_topo import LoadsPhysTopo
except ModuleNotFoundError:
    from features.loads_phys_topo import LoadsPhysTopo

from data_loader.dataset_opf import OPFGraphDataset, make_collate_fn
from trainers.supervised import fit
from trainers.evaluate import evaluate
from gcn_utils.seed import set_seed
from gcn_utils.global_normalize import normalize_inplace
import pickle
import torch 
import numpy as np 
import logging

def setup_logging():
    # é…ç½®æ—¥å¿—æ ¼å¼
    logging.basicConfig(
        level=logging.INFO,  # è®¾ç½®æ—¥å¿—çº§åˆ«
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('/home/goatoine/Documents/Lanyue/models/GNN/run_log/model_outputs.log'),  # ä¿å­˜åˆ°æ–‡ä»¶
        ]
    )
# === the kind of gcn model structure ===
BUILDERS = {
    "original": OriginalGraph,
    # "original_plus_stats": OriginalPlusStats,
    # "chordal": ChordalGraph,
}

def save_checkpoint(model: torch.nn.Module, path: str):
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    torch.save({"model_state": model.state_dict()}, path)
    print(f"[CKPT] Saved to {path}")

def load_checkpoint(model_builder, model_args: dict, path: str, device: str = "cpu") -> torch.nn.Module:
    ckpt = torch.load(path, map_location=device)
    model = model_builder(**model_args)
    model.load_state_dict(ckpt["model_state"])
    model.to(device).eval()
    print(f"[CKPT] Loaded from {path}")
    return model

def build_samples_debug_toy(num=80, minN=8, maxN=28, K=24):
    #è¿™ä¸ªå¦‚æœå†ç”¨è¦+ä¸€ä¸ªå˜é‡y_arr_regss
    out = []
    for i in range(num):
        N = maxN
        # é‚»æ¥çŸ©é˜µ
        A = (torch.rand(N, N) > 0.82).float()
        A = torch.triu(A, 1); A = A + A.t()
        y_reg = torch.randn(K).abs() 
        y_cls = torch.argmin(y_reg).long()
        global_vec = torch.randn(16)
        raw = {
            "A": A,
            "node_load": torch.randn(N, 2),
            "degree": A.sum(dim=1, keepdim=True),
            "global_vec": global_vec,
            "y_reg": y_reg,
            "y_cls": y_cls,
        }
        out.append(raw)
    return out

def convert_samples_to_torch(samples):
    """å°† numpy samples è½¬æ¢ä¸º torch samples"""
    torch_samples = []
    for sample in samples:
        torch_sample = {}
        for key, value in sample.items():
            if key == "A":
                edge_index, edge_weight = value
                torch_sample[key] = (torch.from_numpy(edge_index).long(), torch.from_numpy(edge_weight).float())
            if key == "y_cls":
                    torch_sample[key] = torch.tensor(value).long()
            elif key == "y_reg":
                    torch_sample[key] = torch.tensor(value).float()
            elif isinstance(value, np.ndarray): # y_cls è½¬ä¸º long
                torch_sample[key] = torch.from_numpy(value).float()
            else:
                torch_sample[key] = value
        torch_samples.append(torch_sample)
    return torch_samples

def load_samples_from_npz(npz_path):
    """ä» npz æ–‡ä»¶åŠ è½½ samples"""
    print(f"ğŸ“– Loading samples from {npz_path}")
    data = np.load(npz_path, allow_pickle=True)
    num_samples = int(data['num_samples'])
    
    samples = []
    for i in range(num_samples):
        sample = data[f'sample_{i}'].item()  # .item() å°† numpy å¯¹è±¡è½¬å›å­—å…¸
        samples.append(sample)
    print(f"âœ… Loaded {len(samples)} samples")
    return samples
def main():
    setup_logging()
    cfg = yaml.safe_load(open(CONFIG_PATH, "r"))
    set_seed(cfg.get("seed", 42))
    # samples = build_samples_debug_toy()
    sample_file = "/home/goatoine/Documents/Lanyue/data/data_for_GCN/data_basic_GCN/case2746wop_new_samples.npz"

    samples = load_samples_from_npz(sample_file)
    
    print(f"ğŸ“Š æ€»è®¡: {len(samples)} ä¸ªæ ·æœ¬")
    # è½¬æ¢ä¸º torch tensor
    samples = convert_samples_to_torch(samples)
    # ç»Ÿè®¡ y_cls çš„ç±»åˆ«åˆ†å¸ƒ
    y_cls_values = [sample['y_cls'].item() for sample in samples]
    unique, counts = np.unique(y_cls_values, return_counts=True)
    print(f"ğŸ“ˆ y_cls ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique, counts))}")
    #samples = load_from_csv_or_jld2(...)
    # 3) Setup graph builder and feature pipeline
    builder = BUILDERS[cfg["builder"]]()
    pipeline = LoadsPhysTopo()
    mode_name = cfg.get("global",{}).get("norm","zscore")
    norm = normalize_inplace(samples, mode=mode_name, key="global_vec")

    # 4) split and generate Dataset (ä¸‰ä»½ï¼štrain, val, test)
    # notes that the samples are already shuffled
    train_ratio = 0.6  # 60% è®­ç»ƒ
    val_ratio = 0.2    # 20% éªŒè¯  
    test_ratio = 0.2   # 20% æµ‹è¯•

    train_split = int(train_ratio * len(samples))
    val_split = int((train_ratio + val_ratio) * len(samples))

    train_ds = OPFGraphDataset(samples[:train_split], builder.build, pipeline.node_features, norm)
    val_ds   = OPFGraphDataset(samples[train_split:val_split], builder.build, pipeline.node_features, norm)
    test_ds  = OPFGraphDataset(samples[val_split:], builder.build, pipeline.node_features, norm)

    # simple model, that every graph is has the same size, so we can batch them directly
    # collate_fn is used to merge a list of samples to a batch
    train_ds.collate_fn = make_collate_fn(train_ds)
    val_ds.collate_fn = make_collate_fn(val_ds)
    test_ds.collate_fn = make_collate_fn(test_ds)

    print(f"[INFO] æ•°æ®é›†åˆ†å‰²:")
    print(f"  è®­ç»ƒé›†: {len(train_ds)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_ds)} æ ·æœ¬") 
    print(f"  æµ‹è¯•é›†: {len(test_ds)} æ ·æœ¬")

    # 5) build model
    in_dim = train_ds[0]["X"].shape[1]
    model_args = {
    "name": cfg["model"]["name"],
    "in_dim": in_dim,
    "hidden": cfg["model"].get("hidden", [128, 128]),
    "out_dim": cfg["model"].get("out_dim", 24),
    "dropout": cfg["model"].get("dropout", 0.2),
    }
    if "g_dim" in cfg["model"]:
        model_args["g_dim"] = cfg["model"]["g_dim"]

    model = build_model(**model_args) 

    # ---- è°ƒè¯•å»ºè®®ï¼šé¦–æ¬¡ batch æ‰“å°ä¸€ä¸‹å½¢çŠ¶ï¼Œç¡®è®¤è¾“å…¥æ— è¯¯ ----
    first = train_ds[0]
    print("[dbg] A_hat:", first["A_hat"].shape, "X:", first["X"].shape, "y_reg:", first["y_reg"].shape,"y_arr_reg:", first["y_arr_reg"].shape, "y_cls:", first["y_cls"].shape)

    # 6) è®­ç»ƒ
    fit(
        model,
        train_ds,
        val_ds,
        epochs=cfg["train"].get("epochs", 5),
        batch_size=cfg["train"].get("batch_size", 8),
        lr=cfg["train"].get("lr", 3e-4),
        device="cpu",
    )
    # the metric of Y visualization
    print("Train Done.")
    # === ä¿å­˜æœ€ç»ˆæƒé‡ï¼ˆæœ¬æ¬¡è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ï¼‰ ===
    ckpt_path = "/home/goatoine/Documents/Lanyue/models/GNN/checkpoints/final.pt"
    save_checkpoint(model, ckpt_path)

    # === ï¼ˆæ¼”ç¤ºï¼‰é‡æ–°åŠ è½½æƒé‡å†åšæµ‹è¯•è¯„ä¼° ===
    # è¯´æ˜ï¼šè¿™ä¸€æ­¥æ¨¡æ‹Ÿâ€œå¦ä¸€ä¸ªè¿›ç¨‹/ä¹‹åçš„æ—¶åˆ»â€è¯„ä¼°ï¼›å®é™…ä½¿ç”¨æ—¶ï¼Œä½ å¯ä»¥ä»…ä¿ç•™ load+evaluate çš„éƒ¨åˆ†ã€‚
    reloaded_model = load_checkpoint(build_model, model_args, ckpt_path, device="cpu")


    from trainers.evaluate import evaluate
    
    # ... è®­ç»ƒå®Œæˆåï¼š
    test_metrics = evaluate(
        model=model,
        dataset=test_ds,
        batch_size=cfg["train"].get("batch_size", 8),
        device="cpu",
        save_dir="/home/goatoine/Documents/Lanyue/models/GNN//result/figure",
        # å¯é€‰ï¼šä¼ ç±»åï¼›è‹¥ä¸ä¼ åˆ™ç”¨ 0..C-1
        class_names=[str(i) for i in range(15)],  # æœ€æ–°æ¨¡å‹æ˜¯15ç±»
    )

    print("[TEST] acc:", f'{test_metrics["cls_accuracy_top1"]:.4f}')
    print("[TEST] overall MAE/RMSE:", f'{test_metrics["scalar_reg_mae"]:.6f}', f'{test_metrics["scalar_reg_rmse"]:.6f}')
    print("CM å›¾ç‰‡å·²ä¿å­˜åˆ°: .../result/figure/confusion_matrix.png")


if __name__ == "__main__":
    main()
