from __future__ import annotations
from typing import Literal, Sequence, Any
import torch

NormalizationMode = Literal["zscore", "minmax", "robust", "none"]

class GlobalNormalizer:
    def __init__(self, mode: NormalizationMode = "zscore", eps: float = 1e-8):
        assert mode in ("zscore","minmax","robust","none")
        self.mode, self.eps, self.fitted = mode, eps, False
        self.mean = self.std = self.minv = self.maxv = self.median = self.iqr = None

    def fit(self, X: torch.Tensor):
        X = X.float()
        if self.mode == "none":
            self.fitted = True; return self
        if self.mode == "zscore":
            self.mean = X.mean(0)
            self.std  = X.std(0, unbiased=False).clamp_min(self.eps)
        elif self.mode == "minmax":
            self.minv = X.min(0).values
            self.maxv = X.max(0).values
            same = (self.maxv - self.minv).abs() < self.eps
            self.maxv[same] = self.minv[same] + 1.0
        elif self.mode == "robust":
            self.median = X.median(0).values
            q75, q25 = X.quantile(0.75, dim=0), X.quantile(0.25, dim=0)
            self.iqr = (q75 - q25).clamp_min(self.eps)
        self.fitted = True
        return self

    def transform(self, x: torch.Tensor) -> torch.Tensor:
        x = x.float()
        if (self.mode == "none") or (not self.fitted):
            return x
        if self.mode == "zscore": return (x - self.mean) / self.std
        if self.mode == "minmax": return (x - self.minv) / (self.maxv - self.minv)
        if self.mode == "robust": return (x - self.median) / self.iqr
        return x

class MultiDimNormalizer:
    
    def __init__(self, mode: NormalizationMode = "zscore", eps: float = 1e-8):
        self.mode = mode
        self.eps = eps
        self.fitted = False
        self.normalizers = []  # å­˜å‚¨æ¯ä¸ªç‰¹å¾ç»´åº¦çš„æ ‡å‡†åŒ–å™¨
        self.num_features = None
        self.original_data_shape = None  # è®°å½•å•ä¸ªæ ·æœ¬çš„åŸå§‹å½¢çŠ¶
        
    def fit(self, X: torch.Tensor):
        """
        X: [..., N_features] - æœ€åä¸€ç»´æ˜¯ç‰¹å¾ç»´åº¦
        """
        X = X.float()
        
        # è®°å½•åŸå§‹å½¢çŠ¶ï¼ˆé™¤äº†batchç»´åº¦ï¼‰
        if X.dim() == 1:
            self.original_data_shape = torch.Size([])  # æ ‡é‡
            self.num_features = 1
            X = X.unsqueeze(-1)  # [N] -> [N, 1]
        else:
            self.original_data_shape = X.shape[1:]  # å»æ‰batchç»´åº¦
            self.num_features = X.shape[-1]  # æœ€åä¸€ç»´æ˜¯ç‰¹å¾æ•°
        
        # å°†æ‰€æœ‰éç‰¹å¾ç»´åº¦å±•å¼€æˆæ ·æœ¬ï¼š[..., N_features] -> [total_samples, N_features]
        X_reshaped = X.view(-1, self.num_features)  # [total_samples, N_features]
        
        print(f"ğŸ” MultiDimNormalizer fit: original shape {X.shape} -> reshaped {X_reshaped.shape}")
        print(f"ğŸ” num_features: {self.num_features}, original_data_shape: {self.original_data_shape}")
        
        self.normalizers = []
        
        # ä¸ºæ¯ä¸ªç‰¹å¾ç»´åº¦åˆ›å»ºç‹¬ç«‹çš„æ ‡å‡†åŒ–å™¨
        for dim in range(self.num_features):
            dim_data = X_reshaped[:, dim:dim+1]  # [total_samples, 1]
            norm = GlobalNormalizer(self.mode, self.eps).fit(dim_data)
            self.normalizers.append(norm)
            
        self.fitted = True
        return self
    
    def transform(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: [..., N_features] - å¯ä»¥æ˜¯å•ä¸ªæ ·æœ¬æˆ–æ‰¹é‡æ ·æœ¬
        """
        if not self.fitted:
            return x
            
        x = x.float()
        original_shape = x.shape
        
        # ç¡®ä¿è¾“å…¥çš„ç‰¹å¾ç»´åº¦åŒ¹é…
        if x.shape[-1] != self.num_features:
            raise ValueError(f"Expected {self.num_features} features, got {x.shape[-1]}")
        
        # å°†è¾“å…¥reshapeä¸º [total_samples, N_features]
        x_reshaped = x.view(-1, self.num_features)
        
        # å¯¹æ¯ä¸ªç‰¹å¾ç»´åº¦åˆ†åˆ«å˜æ¢
        transformed_features = []
        for dim in range(self.num_features):
            dim_data = x_reshaped[:, dim:dim+1]  # [total_samples, 1]
            transformed = self.normalizers[dim].transform(dim_data)
            transformed_features.append(transformed.squeeze(-1))  # [total_samples]
        
        # é‡æ–°ç»„åˆç‰¹å¾
        result = torch.stack(transformed_features, -1)  # [total_samples, N_features]
        
        # æ¢å¤åŸå§‹å½¢çŠ¶
        return result.view(original_shape)

    @property 
    def mean(self):
        """è¿”å›æ‰€æœ‰ç‰¹å¾ç»´åº¦çš„å‡å€¼"""
        if not self.fitted or self.mode != "zscore":
            return None
        return torch.tensor([norm.mean.item() for norm in self.normalizers])
    
    @property
    def std(self):
        """è¿”å›æ‰€æœ‰ç‰¹å¾ç»´åº¦çš„æ ‡å‡†å·®"""
        if not self.fitted or self.mode != "zscore":
            return None
        return torch.tensor([norm.std.item() for norm in self.normalizers])

def normalize_inplace(samples: Sequence[dict[str, Any]], *,
                      mode: NormalizationMode = "zscore",
                      key: str = "global_vec",
                      strict: bool = True):
    has_any = any((key in s) for s in samples)
    if not has_any:
        return None
    
    data_list = []
    original_shapes = []
    for i, s in enumerate(samples):
        if key not in s:
            if strict:
                raise ValueError(f"Sample {i} missing '{key}' while others have it.")
            else:
                continue
        # ç¡®ä¿æ•°æ®æ˜¯ float32 å¹¶ä¿æŒåŸå§‹å½¢çŠ¶
        data = torch.as_tensor(s[key], dtype=torch.float32)
        original_shapes.append(data.shape)
        data_list.append(data)  # ä¸å†flattenï¼Œä¿æŒåŸå§‹å½¢çŠ¶

    if not data_list:
        return None
    
    # å †å æ‰€æœ‰æ ·æœ¬ï¼š[N_samples, ...] 
    all_data = torch.stack(data_list, 0)
    print(f"ğŸ” Normalizing {key}: shape {all_data.shape}")
    
    # ä½¿ç”¨å¤šç»´æ ‡å‡†åŒ–å™¨ - æœ€åä¸€ç»´æ˜¯ç‰¹å¾ç»´åº¦
    norm = MultiDimNormalizer(mode).fit(all_data)

    # å¯¹æ¯ä¸ªæ ·æœ¬åº”ç”¨å˜æ¢
    j = 0
    for s in samples:
        if key not in s: 
            continue

        sample_data = all_data[j]  
        transformed = norm.transform(sample_data.unsqueeze(0))  
        s[key] = transformed  
        j += 1
    
    return norm
